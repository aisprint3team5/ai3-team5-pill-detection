# -*- coding: utf-8 -*-
"""데이터셋 구성(DE완료).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fOyCvkzI3HwjllnNMSMnLz2-toHxF-J_

###Kaggle api 등록 (kaggle.json)
"""

# -*- coding: utf-8 -*-

print("kaggle 개인 토큰 파일 (kaggle. json)을 업로드 하셨나요?")

import os
import shutil

answer = input("kaggle 개인 토큰 파일 (kaggle.json)을 구글코랩에 업로드 하셨나요? (Y/N): ")
if answer.lower() == 'y':
    src_path = '/content/kaggle.json'
    dst_dir = '/root/.config/kaggle'
    os.makedirs(dst_dir, exist_ok=True)
    shutil.copy(src_path, os.path.join(dst_dir, 'kaggle.json'))
    print(f"kaggle.json 파일이 {dst_dir}로 복사되었습니다.")
else:
    print("먼저 kaggle.json 파일을 업로드해주세요.")

!chmod 600 /root/.config/kaggle/kaggle.json

"""##깃헙과 연결"""

# 1) 환경 변수로 토큰 설정 (안전)
import os
os.environ['GITHUB_TOKEN'] = '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'


# 2) 토큰을 이용해 브랜치 전체 clone
!git clone https://$GITHUB_TOKEN@github.com/aisprint3team5/ai3-team5-pill-detection.git

!git config --global user.email "selenium01234@gmail.com"
!git config --global user.name  "Seri012"

# Commented out IPython magic to ensure Python compatibility.
###연결된 원격 브랜치 확인
# %cd /content/ai3-team5-pill-detection
!git fetch
!git branch -r
print("==========")
# 또는
!git ls-remote --heads origin

#현재 브랜치 확인
!git branch -a

!git branch

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# 
# 
# # origin에 있는 모든 브랜치 목록 가져오기 (HEAD 제외)
# branches=$(git branch -r | grep origin/ | grep -v HEAD | sed 's# *origin/##')
# 
# for branch in $branches; do
#   echo "=== $branch 체크아웃 및 최신 커밋 업데이트 ==="
#   git fetch origin "$branch"
#   git checkout "$branch"
#   git pull --rebase origin "$branch"
# done
#

branch = "de/data"   # ← change this

##특정 브랜치에 체크아웃(연결)&최신 커밋받아오기
#%cd ai3-team5-pill-detection
!git fetch origin {branch}
!git checkout {branch}
!git pull --rebase origin {branch}

!git branch #다시 현재 연결된 브랜치 위치확인

"""###Colab내에서 생성된 파일들 전부 깃허브에 올리기.(작업디렉토리의 파일들 push)"""

# Colab내 작업 디렉토리에서 생성된 파일들 복사
!cp -r /content/ai3-team5-pill-detection/data *


#cd /content/ai3-team5-pill-detection
destbranch = "el_se/experiment" # ← change this 원하는 브랜치 입력

# el_se/experiment 브랜치로 이동 (이미 체크아웃 돼 있으면 생략 가능)
!git fetch origin {destbranch}
!git checkout {destbranch}

# 변경된 Colab 파일 모두 스테이징
!git add .

# 커밋
!git commit -m "Add Colab-generated Data files"

# 해당 브랜치로 푸시
!git push origin el_se/experiment

"""###케글에서 데이터 받아오기"""

#데이터셋 불러오기
import os
import zipfile
!chmod 600 /root/.config/kaggle/kaggle.json


# 데이터셋 다운로드
!kaggle competitions download -c ai03-level1-project -p ./data

import zipfile
import os

zip_path = "./data/ai03-level1-project.zip"
extract_path = "./data/raw"

# 추출 경로가 없으면 생성
os.makedirs(extract_path, exist_ok=True)

if os.path.exists(zip_path):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)
    print(f"'{zip_path}' 파일이 '{extract_path}' 경로에 성공적으로 압축 해제되었습니다.")
else:
    print(f"'{zip_path}' 파일을 찾을 수 없습니다. 다운로드가 완료되었는지 확인해주세요.")

"""###DE팀의 el/ETL브랜치의 test.py 실행

"""

import os
import torch

from utils.Dataset.Dataset import PillYoloDataset
from utils.Dataset.transformer import PillImageTransform
from utils.Dataset import dataset_parser
from utils.Dataset.dataloader import *
from config.config import CONFIG
from utils.common import *
import random
import yaml


DB_PATH = ROOT_DIR / "script/db/pill_metadata.db"

# Config paths
#CONFIG_INPUT_IMAGE_DIR = CONFIG["paths"]["input_image_dir"]
#CONFIG_INPUT_LABEL_DIR = CONFIG["paths"]["input_label_dir"]

CONFIG_TRAIN           = CONFIG["paths"]["yolo_train_dir"]
CONFIG_TRAIN_LABEL     = CONFIG["paths"]["yolo_train_label"]
CONFIG_VAL             = CONFIG["paths"]["yolo_val_dir"]
CONFIG_VAL_LABEL       = CONFIG["paths"]["yolo_val_label"]
CONFIG_ORIGINAL_DATASET = CONFIG["paths"]["original_dataset"]
CONFIG_TRAIN_IMAGE_DIR = CONFIG["paths"]["train_image_dir"]
CONFIG_TRAIN_ANNO_DIR = CONFIG["paths"]["train_annot_dir"]
#CONFIG_TRAIN_LABEL_DIR = CONFIG["paths"]["train_label_dir"]
CONFIG_SPLIT_DATA_DIR   = CONFIG["paths"]["yolo_data_dir"]
CONFIG_DATA_YAML_PATH = CONFIG["paths"]["data_yaml_path"]
CONFIG_DB_PATH = CONFIG["paths"]["db_path"]
CONFIG_CACHE_DIR = CONFIG["paths"]["pickle_path"]
CONFIG_CLASS_FILE = CONFIG["paths"]["class_names_path"]
# Static paths

# CLASS_FILE      = ROOT_DIR / "data/class_names.txt"
# TRAIN_IMAGE_DIR = ROOT_DIR / "data/raw/train_images"
# TRAIN_ANN_DIR   = ROOT_DIR / "data/raw/train_annotations"
# OUTPUT_IMG_DIR  = ROOT_DIR / "output/contour_detection"
# OUTPUT_DIR      = ROOT_DIR / "output"
# CONFIG_DIR      = ROOT_DIR / "config"
# PICKLE_PATH     = ROOT_DIR / "cache/parsed_dataset.pkl"
# DB_PATH = ROOT_DIR / "script/db/pill_metadata.db"

VAL_SPLIT = 0.1

# if not is_data_downloaded(CONFIG_ORIGINAL_DATASET):
#     print("Data is not downloaded. Downloading...")
#download_data(CONFIG_ORIGINAL_DATASET)
# else:
#     print("Data is already downloaded.")


dataset = convert_to_json(CONFIG_DB_PATH)



 # Parse json file
parser = dataset_parser.PillDatasetParser(CONFIG_TRAIN_IMAGE_DIR, CONFIG_TRAIN_ANNO_DIR)
if os.path.exists(CONFIG_CACHE_DIR):
    dataset = parser.load_from_pickle(CONFIG_CACHE_DIR)
else:
    # Parse and save
    dataset = parser.parse()
    parser.save_to_pickle(CONFIG_CACHE_DIR)


all_class_names = set()
for data in dataset:
    for cat in data["categories"]:
        all_class_names.add(cat["category_name"])
class_names = sorted(list(all_class_names))
class_to_idx = {name: idx for idx, name in enumerate(class_names)}

# #  디렉토리 생성
for split in ["train", "val"]:
    for t in ["images", "labels"]:
        os.makedirs(os.path.join(CONFIG_SPLIT_DATA_DIR, split, t), exist_ok=True)

# # split dataset
random.shuffle(dataset)
val_size = int(len(dataset) * VAL_SPLIT)
val_data = dataset[:val_size]
train_data = dataset[val_size:]

convert_to_yolo(train_data, class_to_idx, CONFIG_TRAIN_IMAGE_DIR, CONFIG_TRAIN, CONFIG_TRAIN_LABEL)
convert_to_yolo(val_data,class_to_idx, CONFIG_TRAIN_IMAGE_DIR, CONFIG_VAL, CONFIG_VAL_LABEL)
# # #
# split after dataset is converted to YOLO format
# split_yolo_dataset( CONFIG_INPUT_IMAGE_DIR,
#             CONFIG_INPUT_LABEL_DIR,
#             CONFIG_OUTPUT_DIR)

data_yaml = {
    "yolo_train_dir": CONFIG_TRAIN,
    "yolo_train_label": CONFIG_TRAIN_LABEL,
    "yolo_val_dir": CONFIG_VAL,
    "yolo_val_label": CONFIG_VAL_LABEL,
    "train": CONFIG_TRAIN,
    "val": CONFIG_VAL,
    "nc": len(class_names),
    "names": class_names
}
#Save data.yaml
with open(CONFIG_DATA_YAML_PATH, "w") as f:
    yaml.dump(data_yaml, f, allow_unicode=True)

class_name_to_idx, _ = load_class_mapping(CONFIG_CLASS_FILE)

# Define transformation
transform = PillImageTransform(resize=(640, 640))

# # Load train and val datasets
train_dataset = PillYoloDataset(
    image_dir=CONFIG_TRAIN,
    label_dir=CONFIG_TRAIN_LABEL,
    class_to_idx=class_name_to_idx,
    S=7, B=2, C=len(class_name_to_idx),
    transform=transform
)
val_dataset = PillYoloDataset(
    image_dir=CONFIG_VAL,
    label_dir=CONFIG_VAL_LABEL,
    class_to_idx=class_name_to_idx,
    transform=transform
)