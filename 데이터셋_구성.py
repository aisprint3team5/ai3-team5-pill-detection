# -*- coding: utf-8 -*-
"""데이터셋 구성.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fVFbOnoDuYUMhjvQOTqqNt93kwrjVMDd

###Kaggle api 등록 (kaggle.json)
"""

# -*- coding: utf-8 -*-

print("kaggle 개인 토큰 파일 (kaggle. json)을 업로드 하셨나요?")

import os
import shutil

answer = input("kaggle 개인 토큰 파일 (kaggle.json)을 구글코랩에 업로드 하셨나요? (Y/N): ")
if answer.lower() == 'y':
    src_path = '/content/kaggle.json'
    dst_dir = '/root/.config/kaggle'
    os.makedirs(dst_dir, exist_ok=True)
    shutil.copy(src_path, os.path.join(dst_dir, 'kaggle.json'))
    print(f"kaggle.json 파일이 {dst_dir}로 복사되었습니다.")
else:
    print("먼저 kaggle.json 파일을 업로드해주세요.")

"""##깃헙과 연결"""

# 1) 환경 변수로 토큰 설정 (안전)
import os
os.environ['GITHUB_TOKEN'] = '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'


# 2) 토큰을 이용해 브랜치 전체 clone
!git clone https://$GITHUB_TOKEN@github.com/aisprint3team5/ai3-team5-pill-detection.git

!git config --global user.email "selenium01234@gmail.com"
!git config --global user.name  "Seri012"

# Commented out IPython magic to ensure Python compatibility.
###연결된 원격 브랜치 확인
# %cd /content/ai3-team5-pill-detection
!git fetch
!git branch -r
print("==========")
# 또는
!git ls-remote --heads origin

#현재 브랜치 확인
!git branch -a

!git branch

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# 
# 
# # origin에 있는 모든 브랜치 목록 가져오기 (HEAD 제외)
# branches=$(git branch -r | grep origin/ | grep -v HEAD | sed 's# *origin/##')
# 
# for branch in $branches; do
#   echo "=== $branch 체크아웃 및 최신 커밋 업데이트 ==="
#   git fetch origin "$branch"
#   git checkout "$branch"
#   git pull --rebase origin "$branch"
# done
#

branch = "de/data"   # ← change this

##특정 브랜치에 체크아웃(연결)&최신 커밋받아오기
#%cd ai3-team5-pill-detection
!git fetch origin {branch}
!git checkout {branch}
!git pull --rebase origin {branch}

!git branch #다시 현재 연결된 브랜치 위치확인

"""###Colab내에서 생성된 파일들 전부 깃허브에 올리기.(작업디렉토리의 파일들 push)"""

# Colab내 작업 디렉토리에서 생성된 파일들 복사
!cp -r /content/ai3-team5-pill-detection/data *


#cd /content/ai3-team5-pill-detection
destbranch = "el_se/experiment" # ← change this 원하는 브랜치 입력

# el_se/experiment 브랜치로 이동 (이미 체크아웃 돼 있으면 생략 가능)
!git fetch origin {destbranch}
!git checkout {destbranch}

# 변경된 Colab 파일 모두 스테이징
!git add .

# 커밋
!git commit -m "Add Colab-generated Data files"

# 해당 브랜치로 푸시
!git push origin el_se/experiment

"""###케글에서 데이터 받아오기"""

#데이터셋 불러오기
import os
import zipfile
!chmod 600 /root/.config/kaggle/kaggle.json


# 데이터셋 다운로드
!kaggle competitions download -c ai03-level1-project -p ./data

import zipfile
import os

zip_path = "./data/ai03-level1-project.zip"
extract_path = "./data/raw"

# 추출 경로가 없으면 생성
os.makedirs(extract_path, exist_ok=True)

if os.path.exists(zip_path):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)
    print(f"'{zip_path}' 파일이 '{extract_path}' 경로에 성공적으로 압축 해제되었습니다.")
else:
    print(f"'{zip_path}' 파일을 찾을 수 없습니다. 다운로드가 완료되었는지 확인해주세요.")

"""###DE팀의 el/ETL브랜치의 test.py 실행

"""

from pathlib import Path
import os, random, yaml, sys

# 1) 프로젝트 루트와 실험 디렉토리 정의
PROJECT_ROOT   = Path("/content/ai3-team5-pill-detection").resolve()
EXPERIMENT_DIR = PROJECT_ROOT / "experiments"

# 2) CONFIG(=config/config.yml)에서 상대경로 불러오기
#    (이미 import 되어 있다면 이 부분 건너뛰세요)
# import yaml
# CONFIG = yaml.safe_load(open(PROJECT_ROOT/"config/config.yml", encoding="utf-8"))

# 3) 입력 데이터 디렉토리 (원본): PROJECT_ROOT + 상대경로
CONFIG_INPUT_IMAGE_DIR = PROJECT_ROOT / CONFIG["paths"]["input_image_dir"]
CONFIG_INPUT_LABEL_DIR = PROJECT_ROOT / CONFIG["paths"]["input_label_dir"]

# 4) 출력 데이터 디렉토리 (실험 결과): experiments 폴더 하위로 강제 오버라이드
CONFIG_OUTPUT_DIR      = EXPERIMENT_DIR
CONFIG_TRAIN           = EXPERIMENT_DIR / "train" / "images"
CONFIG_TRAIN_LABEL     = EXPERIMENT_DIR / "train" / "labels"
CONFIG_VAL             = EXPERIMENT_DIR / "val"   / "images"
CONFIG_VAL_LABEL       = EXPERIMENT_DIR / "val"   / "labels"

# 5) 기타 경로 (캐시, DB, 클래스 파일 등)
CLASS_FILE    = PROJECT_ROOT / "YOLO_dataset/class_names.txt"
TRAIN_IMAGE_DIR = PROJECT_ROOT / "data/raw/train_images"
TRAIN_ANN_DIR   = PROJECT_ROOT / "data/raw/train_annotations"
CONFIG_DIR      = PROJECT_ROOT / "config"
PICKLE_PATH     = PROJECT_ROOT / "cache/parsed_dataset.pkl"
DB_PATH         = PROJECT_ROOT / "script/db/pill_metadata.db"

VAL_SPLIT = 0.1

# (필요 시) Utils 모듈 임포트 경로 추가
sys.path.append(str(PROJECT_ROOT))

# 디렉토리 생성
for split in ["train", "val"]:
    for t in ["images", "labels"]:
        os.makedirs(EXPERIMENT_DIR/ split / t, exist_ok=True)
os.makedirs(CONFIG_DIR, exist_ok=True)

# 데이터 분할
random.shuffle(dataset)
val_size  = int(len(dataset)*VAL_SPLIT)
val_data  = dataset[:val_size]
train_data= dataset[val_size:]

# YOLO 포맷 생성
convert_to_yolo(train_data, TRAIN_IMAGE_DIR, CONFIG_TRAIN, CONFIG_TRAIN_LABEL)
convert_to_yolo(val_data,   TRAIN_IMAGE_DIR, CONFIG_VAL,   CONFIG_VAL_LABEL)

# 실제 파일 이동 (split_yolo_dataset 내부에서 CONFIG_OUTPUT_DIR 하위로 저장)
split_yolo_dataset(
    CONFIG_INPUT_IMAGE_DIR,
    CONFIG_INPUT_LABEL_DIR,
    CONFIG_OUTPUT_DIR
)

# data.yaml 덮어쓰기
data_yaml = {
    "yolo_train_dir":  str(CONFIG_TRAIN),
    "yolo_train_label":str(CONFIG_TRAIN_LABEL),
    "yolo_val_dir":    str(CONFIG_VAL),
    "yolo_val_label":  str(CONFIG_VAL_LABEL),
    "nc": len(class_names),
    "names": class_names
}
with open(CONFIG_DIR/"data.yaml","w") as f:
    yaml.dump(data_yaml, f, allow_unicode=True)

# create_schema.py
import sqlite3

DB_PATH = "pill_metadata.db"

def create_db():
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("""
    CREATE TABLE IF NOT EXISTS pill_metadata (
        category_id INTEGER PRIMARY KEY,
        name TEXT,
        name_en TEXT,
        shape TEXT,
        company TEXT,
        material TEXT,
        color TEXT,
        form_code_name TEXT,
        drug_S TEXT,
        dl_mapping_code TEXT,
        img_key TEXT,
        print_front TEXT,
        print_back TEXT
    )
    """)
    conn.commit()
    conn.close()

create_db()

#Library
import os
import random
import shutil
import yaml
from pathlib import Path

import torch
from torchvision import transforms

import Utils.common
# Custom modules
from Utils.Datasets.Dataset import PillYoloDataset
from Utils.Datasets.dataloader import *
from Utils.preprocessing import *
from Utils.common import split_yolo_dataset
from Utils.Datasets.dataset_parser import PillDatasetParser
from Utils.Datasets.transformer import PillImageTransform
from config.config import CONFIG


from pathlib import Path
import yaml, os, sys

# 1) Colab 기준 루트를 experiments 폴더 부모로 설정
ROOT_DIR = Path("/content/ai3-team5-pill-detection/experiments").resolve().parent
#   → /content/ai3-team5-pill-detection

# 2) YAML 불러오기
cfg = yaml.safe_load(open(ROOT_DIR / "config/config.yaml", encoding="utf-8"))


# 5) 필요하면 PYTHONPATH에 추가
sys.path.append(str(ROOT_DIR))

# Constants & Paths

ROOT_DIR = Path("/content/ai3-team5-pill-detection/experiments").resolve().parent

# Config paths
CONFIG_INPUT_IMAGE_DIR = CONFIG["paths"]["input_image_dir"]
CONFIG_INPUT_LABEL_DIR = CONFIG["paths"]["input_label_dir"]
CONFIG_OUTPUT_DIR      = CONFIG["paths"]["output_dir"]
CONFIG_TRAIN           = CONFIG["paths"]["yolo_train_dir"]
CONFIG_TRAIN_LABEL     = CONFIG["paths"]["yolo_train_label"]
CONFIG_VAL             = CONFIG["paths"]["yolo_val_dir"]
CONFIG_VAL_LABEL       = CONFIG["paths"]["yolo_val_label"]

# Static paths
INPUT_IMAGE_DIR = ROOT_DIR / "YOLO_dataset/images"
INPUT_LABEL_DIR = ROOT_DIR / "YOLO_dataset/labels"
CLASS_FILE      = ROOT_DIR / "YOLO_dataset/class_names.txt"
TRAIN_IMAGE_DIR = ROOT_DIR / "data/raw/train_images"
TRAIN_ANN_DIR   = ROOT_DIR / "data/raw/train_annotations"
OUTPUT_IMG_DIR  = ROOT_DIR / "output/contour_detection"
OUTPUT_DIR      = ROOT_DIR / "output"
CONFIG_DIR      = ROOT_DIR / "config"
PICKLE_PATH     = ROOT_DIR / "cache/parsed_dataset.pkl"
DB_PATH = ROOT_DIR / "script/db/pill_metadata.db"

VAL_SPLIT = 0.1
# Load and explore
#download_data()

#dataset = convert_to_json(DB_PATH)

#
# #
# # # Step 1 : Parse json file
parser = PillDatasetParser(TRAIN_IMAGE_DIR, TRAIN_ANN_DIR)
if os.path.exists(PICKLE_PATH):
    dataset = parser.load_from_pickle(PICKLE_PATH)
else:
    # Parse and save
    dataset = parser.parse()
    parser.save_to_pickle(PICKLE_PATH)
# #
# # #print(dataset[0])
# #
# # # Step 2 :
# #
# # # 1. 클래스 이름 추출 (dataset 구조 기반)
all_class_names = set()
for data in dataset:
    for cat in data["categories"]:
        all_class_names.add(cat["category_name"])
class_names = sorted(list(all_class_names))
class_to_idx = {name: idx for idx, name in enumerate(class_names)}
# #
# # # 2. 디렉토리 생성
for split in ["train", "val"]:
    for t in ["images", "labels"]:
        os.makedirs(os.path.join(CONFIG_OUTPUT_DIR, split, t), exist_ok=True)

os.makedirs(CONFIG_DIR, exist_ok=True)
# #
# # # split dataset
random.shuffle(dataset)
val_size = int(len(dataset) * VAL_SPLIT)
val_data = dataset[:val_size]
train_data = dataset[val_size:]
# #
#
convert_to_yolo(train_data, TRAIN_IMAGE_DIR, CONFIG_TRAIN, CONFIG_TRAIN_LABEL)

convert_to_yolo(val_data, TRAIN_IMAGE_DIR, CONFIG_VAL, CONFIG_VAL_LABEL)
# # #
# # # # 실제 파일 이동
split_yolo_dataset( CONFIG_INPUT_IMAGE_DIR,
            CONFIG_INPUT_LABEL_DIR,
            CONFIG_OUTPUT_DIR)
# #
data_yaml = {
    "yolo_train_dir": str(CONFIG_OUTPUT_DIR / "train" / "images"),
    "yolo_train_label": str(CONFIG_OUTPUT_DIR / "train" / "labels"),
    "yolo_val_dir": str(CONFIG_OUTPUT_DIR / "val" / "images"),
    "yolo_val_label": str(CONFIG_OUTPUT_DIR / "val" / "labels"),
    "nc": len(class_names),
    "names": class_names
}
#
with open(CONFIG_DIR / "data.yaml", "w") as f:
    yaml.dump(data_yaml, f, allow_unicode=True)
#
# # print(f"YOLO 데이터셋 생성 완료\n 클래스 수: {len(class_names)}")
#
class_name_to_idx, _ = Utils.common.load_class_mapping(CLASS_FILE)
#
# # Define transformation
transform = PillImageTransform(resize=(640, 640))
#
# # Load train and val datasets
train_dataset = PillYoloDataset(
    image_dir=CONFIG_TRAIN,
    label_dir=CONFIG_TRAIN_LABEL,
    class_to_idx=class_name_to_idx,
    S=7, B=2, C=len(class_name_to_idx),
    transform=transform
)
val_dataset = PillYoloDataset(
    image_dir=CONFIG_VAL,
    label_dir=CONFIG_VAL_LABEL,
    class_to_idx=class_name_to_idx,
    transform=transform
)




class_txt_path = "YOLO_dataset/class_names.txt"
existed_label_dir = "existed_label"
new_label_dir = "data/new_anno"
output_label_dir = "output_label"

class_name_to_idx, _ = Utils.common.load_class_mapping(class_txt_path)
# === Run ===
Utils.common.merge_labels_with_db(
     CONFIG_TRAIN_LABEL,
     new_label_dir,
     output_label_dir,
     class_name_to_idx,
     DB_PATH
 )
json_files = get_all_annotation_files(TRAIN_ANN_DIR)
print(f"총 {len(json_files)}개의 annotation 파일을 찾았습니다.")
df_annotations = analyze_pill_annotations(json_files)

class_counts = plot_class_distribution(data, top_n=30)
analyze_image_sizes(data)
metadata_and_annotation_analysis(data)

detect_missing_pills(
     dataset=data,
     train_img_dir=TRAIN_IMG_DIR,
     output_dir=OUTPUT_IMG_DIR
 )

#Library
import os
import random
import shutil
import yaml
from pathlib import Path

import torch
from torchvision import transforms

import Utils.common
# Custom modules
from Utils.Datasets.Dataset import PillYoloDataset
from Utils.Datasets.dataloader import *
from Utils.preprocessing import *
from Utils.common import split_yolo_dataset
from Utils.Datasets.dataset_parser import PillDatasetParser
from Utils.Datasets.transformer import PillImageTransform
from config.config import CONFIG



# Constants & Paths

ROOT_DIR = Path().resolve().parent

# Config paths
CONFIG_INPUT_IMAGE_DIR = CONFIG["paths"]["input_image_dir"]
CONFIG_INPUT_LABEL_DIR = CONFIG["paths"]["input_label_dir"]
CONFIG_OUTPUT_DIR      = CONFIG["paths"]["output_dir"]
CONFIG_TRAIN           = CONFIG["paths"]["yolo_train_dir"]
CONFIG_TRAIN_LABEL     = CONFIG["paths"]["yolo_train_label"]
CONFIG_VAL             = CONFIG["paths"]["yolo_val_dir"]
CONFIG_VAL_LABEL       = CONFIG["paths"]["yolo_val_label"]

# Static paths
INPUT_IMAGE_DIR = ROOT_DIR / "YOLO_dataset/images"
INPUT_LABEL_DIR = ROOT_DIR / "YOLO_dataset/labels"
CLASS_FILE      = ROOT_DIR / "YOLO_dataset/class_names.txt"
TRAIN_IMAGE_DIR = ROOT_DIR / "data/raw/train_images"
TRAIN_ANN_DIR   = ROOT_DIR / "data/raw/train_annotations"
OUTPUT_IMG_DIR  = ROOT_DIR / "output/contour_detection"
OUTPUT_DIR      = ROOT_DIR / "output"
CONFIG_DIR      = ROOT_DIR / "config"
PICKLE_PATH     = ROOT_DIR / "cache/parsed_dataset.pkl"
DB_PATH = ROOT_DIR / "script/db/pill_metadata.db"

VAL_SPLIT = 0.1
# Load and explore
#download_data()

#dataset = convert_to_json(DB_PATH)

#
# #
# # # Step 1 : Parse json file
parser = PillDatasetParser(TRAIN_IMAGE_DIR, TRAIN_ANN_DIR)
if os.path.exists(PICKLE_PATH):
    dataset = parser.load_from_pickle(PICKLE_PATH)
else:
    # Parse and save
    dataset = parser.parse()
    parser.save_to_pickle(PICKLE_PATH)
# #
# # #print(dataset[0])
# #
# # # Step 2 :
# #
# # # 1. 클래스 이름 추출 (dataset 구조 기반)
all_class_names = set()
for data in dataset:
    for cat in data["categories"]:
        all_class_names.add(cat["category_name"])
class_names = sorted(list(all_class_names))
class_to_idx = {name: idx for idx, name in enumerate(class_names)}
# #
# # # 2. 디렉토리 생성
for split in ["train", "val"]:
    for t in ["images", "labels"]:
        os.makedirs(os.path.join(CONFIG_OUTPUT_DIR, split, t), exist_ok=True)

os.makedirs(CONFIG_DIR, exist_ok=True)
# #
# # # split dataset
random.shuffle(dataset)
val_size = int(len(dataset) * VAL_SPLIT)
val_data = dataset[:val_size]
train_data = dataset[val_size:]
# #
#
convert_to_yolo(train_data, TRAIN_IMAGE_DIR, CONFIG_TRAIN, CONFIG_TRAIN_LABEL)
convert_to_yolo(val_data, TRAIN_IMAGE_DIR, CONFIG_VAL, CONFIG_VAL_LABEL)
# # #
# # # # 실제 파일 이동
split_yolo_dataset( CONFIG_INPUT_IMAGE_DIR,
            CONFIG_INPUT_LABEL_DIR,
            CONFIG_OUTPUT_DIR)
# #
data_yaml = {
    "yolo_train_dir": str(CONFIG_OUTPUT_DIR / "train" / "images"),
    "yolo_train_label": str(CONFIG_OUTPUT_DIR / "train" / "labels"),
    "yolo_val_dir": str(CONFIG_OUTPUT_DIR / "val" / "images"),
    "yolo_val_label": str(CONFIG_OUTPUT_DIR / "val" / "labels"),
    "nc": len(class_names),
    "names": class_names
}
#
with open(CONFIG_DIR / "data.yaml", "w") as f:
    yaml.dump(data_yaml, f, allow_unicode=True)
#
# # print(f"YOLO 데이터셋 생성 완료\n 클래스 수: {len(class_names)}")
#
class_name_to_idx, _ = Utils.common.load_class_mapping(CLASS_FILE)
#
# # Define transformation
transform = PillImageTransform(resize=(640, 640))
#
# # Load train and val datasets
train_dataset = PillYoloDataset(
    image_dir=CONFIG_TRAIN,
    label_dir=CONFIG_TRAIN_LABEL,
    class_to_idx=class_name_to_idx,
    S=7, B=2, C=len(class_name_to_idx),
    transform=transform
)
val_dataset = PillYoloDataset(
    image_dir=CONFIG_VAL,
    label_dir=CONFIG_VAL_LABEL,
    class_to_idx=class_name_to_idx,
    transform=transform
)




class_txt_path = "YOLO_dataset/class_names.txt"
existed_label_dir = "existed_label"
new_label_dir = "data/new_anno"
output_label_dir = "output_label"

class_name_to_idx, _ = Utils.common.load_class_mapping(class_txt_path)
# === Run ===
Utils.common.merge_labels_with_db(
     CONFIG_TRAIN_LABEL,
     new_label_dir,
     output_label_dir,
     class_name_to_idx,
     DB_PATH
 )
json_files = get_all_annotation_files(TRAIN_ANN_DIR)
print(f"총 {len(json_files)}개의 annotation 파일을 찾았습니다.")
df_annotations = analyze_pill_annotations(json_files)

class_counts = plot_class_distribution(data, top_n=30)
analyze_image_sizes(data)
metadata_and_annotation_analysis(data)

detect_missing_pills(
     dataset=data,
     train_img_dir=TRAIN_IMG_DIR,
     output_dir=OUTPUT_IMG_DIR
 )



#Dataloader.py
import os
import json
from PIL import Image, ImageDraw
from collections import defaultdict
#from sklearn.preprocessing import LabelEncoder
import sqlite3
import matplotlib.pyplot as plt
#import script.db.insert_pills as db

from pathlib import Path


ROOT_DIR = Path().resolve().parent.parent.parent
IMAGE_DIR = ROOT_DIR / "YOLO_dataset/images"
LABEL_DIR = ROOT_DIR / "YOLO_dataset/labels"

OUTPUT_DIR = ROOT_DIR/"outputs/with_bounding_boxes"

TRAIN_IMG_DIR = ROOT_DIR / "data/raw/train_images"
TRAIN_ANN_DIR = ROOT_DIR / "data/raw/train_annotations"
CLASS_FILE = ROOT_DIR / "data/class_names.txt"
YOLO_IMG_DIR = "YOLO_dataset/images"
YOLO_LABEL_DIR = "YOLO_dataset/labels"

def convert_to_yolo(dataset, image_root_dir, output_img_dir, output_label_dir):
    """
       Converts a dataset to YOLO format.

       Args:
           dataset (list): List of parsed annotation dictionaries.
           image_root_dir (str or Path): Directory where original images are stored.
           output_img_dir (str or Path): Directory to save YOLO images.
           output_label_dir (str or Path): Directory to save YOLO-format label files.
       """
    # os.makedirs(output_img_dir, exist_ok=True)
    # os.makedirs(output_label_dir, exist_ok=True)

    # # Build class_id map
    all_class_names = set()
    for data in dataset:
        for category in data["categories"]:
            all_class_names.add(category["category_name"]) # category_name
    class_name_to_id = {name: idx for idx, name in enumerate(sorted(all_class_names))}

    # Convert each image & label
    for data in dataset:
        image_filename = data["image_file"]
        image_path = os.path.join(image_root_dir, image_filename)

        try:
            image = Image.open(image_path).convert("RGB")
        except FileNotFoundError:
            print(f"[Warning] Image not found: {image_path}")
            continue

        width, height = image.size
        label_lines = []

        for category in data["categories"]:
            class_name = category["category_name"]
            class_id = class_name_to_id[class_name]

            for annotation in category["annotations"]:
                x, y, w, h = annotation["bbox"]

                # YOLO format: center_x, center_y, width, height (normalized)
                x_center = (x + w / 2) / width
                y_center = (y + h / 2) / height
                w_norm = w / width
                h_norm = h / height

                line = f"{class_id} {x_center:.6f} {y_center:.6f} {w_norm:.6f} {h_norm:.6f}"
                label_lines.append(line)

        # Save image
        output_img_path = os.path.join(output_img_dir, image_filename)
        image.save(output_img_path)

        #  label
        txt_filename = os.path.splitext(image_filename)[0] + ".txt"
        label_path = os.path.join(output_label_dir, txt_filename)
        with open(label_path, "w") as f:
            f.write("\n".join(label_lines))

        print(f"[Saved] {image_filename} -> YOLO format with {len(label_lines)} boxes")


    if not os.path.exists(CLASS_FILE):
        with open(CLASS_FILE, "w", encoding="utf-8") as f:
            for name, idx in sorted(class_name_to_id.items(), key=lambda x: x[1]):
                f.write(f"{idx}: {name}\n")
    else:
        print(f"[INFO] '{CLASS_FILE}' already exists. Skipping class file creation.")


def convert_to_json(db_path):

   # os.makedirs(OUTPUT_DIR, exist_ok=True)

    dataset = []

    image_id_map = {}
    image_counter = 0
    annotation_counter = 0
    not_found = 0
    # Map image file name to JSON data
    imgfile_to_jsons = defaultdict(list)
    conn  = sqlite3.connect(db_path)

    for root, _, files in os.walk(TRAIN_ANN_DIR):
        for file in files:
            if not file.endswith(".json"):
                continue
            json_path = os.path.join(root, file)
            try:
                with open(json_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
            except Exception as e:
                print(f"Error loading {json_path}: {e}")
                continue

            if not data.get("images") or not data.get("annotations") or not data.get("categories"):
                not_found +=1
                continue

            imgfile = data["images"][0].get("imgfile")
            if imgfile:
                imgfile_to_jsons[imgfile].append(data)


    image_files = [f for f in os.listdir(TRAIN_IMG_DIR) if f.endswith(".png")]

    # Match image files to JSONs
    for imgfile in image_files:
        if imgfile not in imgfile_to_jsons:
            continue

        if imgfile not in image_id_map:
            image_id_map[imgfile] = image_counter
            image_counter += 1

        image_id = image_id_map[imgfile]
        record = {
            "image_file": imgfile,
            "image_id": image_id,
            "categories": []
        }

        category_map = {}

        for data in imgfile_to_jsons[imgfile]:
            image_info = data["images"][0]
            category_info = data["categories"][0]
            category_id = category_info["id"]
            category_name = category_info["name"]
          #  db.insert_category_to_db(conn, category_id, category_name)
            if category_id not in category_map:
                category_map[category_id] = {
                    "category_id": category_id,
                    "category_name": category_name,
                    "annotations": []
                }

            for ann in data["annotations"]:
                bbox = ann.get("bbox")
                if not bbox or not isinstance(bbox, list) or len(bbox) != 4:
                    continue  # skip invalid

                annotation = {
                    "annotation_id": annotation_counter,
                    "bbox": bbox,
                    "iscrowd": ann.get("iscrowd", 0),
                    "area": ann.get("area", 0),
                    "ignore": ann.get("ignore", 0),
                    "images": image_info
                }

                category_map[category_id]["annotations"].append(annotation)
                annotation_counter += 1


        record["categories"] = list(category_map.values())
        dataset.append(record)
    print(f'No annotation {not_found} images')
    # Save to file
    # with open("pill_dataset_structured.json", "w", encoding="utf-8") as f:
    #     json.dump(dataset, f, ensure_ascii=False, indent=2)


    # # Load image
    # for idx, data in enumerate(dataset):
    #     image_filename = data["image_file"]
    #     image_path = os.path.join(TRAIN_IMG_DIR, image_filename)
    #
    #     # Load image
    #     try:
    #         image = Image.open(image_path).convert("RGB")
    #     except FileNotFoundError:
    #         print(f"[Warning] Image not found: {image_path}")
    #         continue
    #
    # # Create figure and axis
    #     fig, ax = plt.subplots(1, figsize=(10, 12))
    #     ax.imshow(image)
    #
    #     # Draw bounding boxes
    #     for category in data["categories"]:
    #         name = category["category_name"]
    #         for annotation in category["annotations"]:
    #             x, y, w, h = annotation["bbox"]
    #             rect = patches.Rectangle((x, y), w, h, linewidth=2,
    #                                      edgecolor='red', facecolor='none')
    #             ax.add_patch(rect)
    #             ax.text(x, y - 5, name, fontsize=10, color='white',
    #                     bbox=dict(facecolor='red', alpha=0.5))
    #
    #     ax.axis('off')
    #     output_path = os.path.join(OUTPUT_DIR, f"annotated_{idx}_{image_filename}")
    #     plt.savefig(output_path, bbox_inches='tight', pad_inches=0)
    #     plt.close(fig)  # Close the figure to free memory
    #
    #     print(f"[Saved] {output_path}")
    conn.close()
    return dataset

def get_all_annotation_files(root_folder):
    json_files = []
    for root, _, files in os.walk(root_folder):
        for file in files:
            if file.endswith(".json"):
                json_files.append(os.path.join(root, file))
    return json_files

# def analyze_pill_annotations(json_paths):
#     records = []
#     for path in tqdm(json_paths, desc="Parsing JSON files"):
#         with open(path, 'r', encoding='utf-8') as f:
#             data = json.load(f)
#
#         image_info = {img["id"]: img for img in data.get("images", [])}
#         categories = {cat["id"]: cat["name"] for cat in data.get("categories", [])}
#
#         for ann in data.get("annotations", []):
#             img = image_info.get(ann["image_id"], {})
#             if not img:
#                 continue
#
#             record = {
#                 "file_name": img.get("file_name"),
#                 "width": img.get("width"),
#                 "height": img.get("height"),
#                 "camera_la": img.get("camera_la"),
#                 "camera_lo": img.get("camera_lo"),
#                 "size": img.get("size"),
#                 "drug_S": img.get("drug_S"),
#                 "dl_name": img.get("dl_name"),
#                 "dl_name_en": img.get("dl_name_en"),
#                 "di_class_no": img.get("di_class_no"),
#                 "drug_shape": img.get("drug_shape", None),
#                 "thick": img.get("thick", None),
#                 "leng_short":  img.get("leng_short", None),
#                 "leng_long":  img.get("leng_long", None),
#                 "back_color": img.get("back_color"),
#                 "drug_dir": img.get("drug_dir"),
#                 "light_color": img.get("light_color"),
#                 "bbox_x": ann["bbox"][0],
#                 "bbox_y": ann["bbox"][1],
#                 "bbox_w": ann["bbox"][2],
#                 "bbox_h": ann["bbox"][3],
#                 "bbox_area": ann["bbox"][2] * ann["bbox"][3],
#                 "category_id": ann["category_id"],
#                 "category_name": categories.get(ann["category_id"], "Unknown")
#             }
#             records.append(record)
#
#     df = pd.DataFrame(records)
#
#     return df

def load_training_data(image_dir, annotation_root):
    """
    Loads image paths and matching annotation JSONs from nested structure.
    - image_dir: path to train_images/
    - annotation_root: path to train_annotations/
    """
    data = []

    for image_filename in os.listdir(image_dir):
        if not image_filename.endswith(".png"):
            continue

        image_path = os.path.join(image_dir, image_filename)
        base_name = image_filename.replace(".png", "")

        #'K-003351-016688-029543-044199_0_2_0_2_75_000_200' → 'K-003351-016688-029543-044199'
        base_id = base_name.split('_')[0]
        annotation_base_dir = os.path.join(annotation_root, base_id)

        annotation_base_dir = "{}_json".format(annotation_base_dir)
        found = False

        if os.path.exists(annotation_base_dir):
            for subdir in os.listdir(annotation_base_dir):
                json_path = os.path.join(annotation_base_dir, subdir, f"{base_name}.json")
                print(json_path)
                if os.path.exists(json_path):
                    with open(json_path, "r") as f:
                        annotation = json.load(f)
                    data.append({
                        "image_path": image_path,
                        "annotation": annotation
                    })
                    found = True
                    break

        if not found:
            print(f"[WARNING] No annotation found for: {image_filename}")
    print(f"length : {len(data)}")

    return data




def is_valid_annotation(annotation):
    if isinstance(annotation, dict):
        print('xx')
        return  "bbox" in annotation and isinstance(annotation["bbox"], list) and len(annotation["bbox"]) == 4
    elif isinstance(annotation, list):
        print('yy')
        return any(
            "bbox" in ann and isinstance(ann["bbox"], list) and len(ann["bbox"]) == 4
            for ann in annotation
        )
    return False

def explore_dataset(data, limit=5, draw_bbox=True):
    count = 0
    for entry in data:
        image_path = entry["image_path"]
        annotation = entry["annotation"]

        if not is_valid_annotation(annotation):
            continue  # Skip invalid annotations

        try:
            image = Image.open(image_path).convert("RGB")
        except Exception as e:
            print(f"Could not open image {image_path}: {e}")
            continue

        if draw_bbox:
            draw = ImageDraw.Draw(image)
            x, y, w, h = annotation["bbox"]
            draw.rectangle([x, y, x + w, y + h], outline="red", width=2)

        # Show image
        plt.figure(figsize=(6, 6))
        plt.imshow(image)
        plt.title(f"{image_path.split('/')[-1]}")
        plt.axis("off")
        plt.show()

        count += 1
        if count >= limit:
            break

#Data.py
#
# #
# # # Step 1 : Parse json file
parser = PillDatasetParser(TRAIN_IMAGE_DIR, TRAIN_ANN_DIR)
if os.path.exists(PICKLE_PATH):
    dataset = parser.load_from_pickle(PICKLE_PATH)
else:
    # Parse and save
    dataset = parser.parse()
    parser.save_to_pickle(PICKLE_PATH)
# #
# # #print(dataset[0])
# #
# # # Step 2 :
# #
# # # 1. 클래스 이름 추출 (dataset 구조 기반)
all_class_names = set()
for data in dataset:
    for cat in data["categories"]:
        all_class_names.add(cat["category_name"])
class_names = sorted(list(all_class_names))
class_to_idx = {name: idx for idx, name in enumerate(class_names)}
# #
# # # 2. 디렉토리 생성
for split in ["train", "val"]:
    for t in ["images", "labels"]:
        os.makedirs(os.path.join(CONFIG_OUTPUT_DIR, split, t), exist_ok=True)

os.makedirs(CONFIG_DIR, exist_ok=True)
# #
# # # split dataset
random.shuffle(dataset)
val_size = int(len(dataset) * VAL_SPLIT)
val_data = dataset[:val_size]
train_data = dataset[val_size:]
# #
#
convert_to_yolo(train_data, TRAIN_IMAGE_DIR, CONFIG_TRAIN, CONFIG_TRAIN_LABEL)
convert_to_yolo(val_data, TRAIN_IMAGE_DIR, CONFIG_VAL, CONFIG_VAL_LABEL)
# # #
# # # # 실제 파일 이동
split_yolo_dataset( CONFIG_INPUT_IMAGE_DIR,
            CONFIG_INPUT_LABEL_DIR,
            CONFIG_OUTPUT_DIR)
# #
data_yaml = {
    "yolo_train_dir": str(CONFIG_OUTPUT_DIR / "train" / "images"),
    "yolo_train_label": str(CONFIG_OUTPUT_DIR / "train" / "labels"),
    "yolo_val_dir": str(CONFIG_OUTPUT_DIR / "val" / "images"),
    "yolo_val_label": str(CONFIG_OUTPUT_DIR / "val" / "labels"),
    "nc": len(class_names),
    "names": class_names
}
#
with open(CONFIG_DIR / "data.yaml", "w") as f:
    yaml.dump(data_yaml, f, allow_unicode=True)
#
# # print(f"YOLO 데이터셋 생성 완료\n 클래스 수: {len(class_names)}")
#
class_name_to_idx, _ = Utils.common.load_class_mapping(CLASS_FILE)
#
# # Define transformation
transform = PillImageTransform(resize=(640, 640))
#
# # Load train and val datasets
train_dataset = PillYoloDataset(
    image_dir=CONFIG_TRAIN,
    label_dir=CONFIG_TRAIN_LABEL,
    class_to_idx=class_name_to_idx,
    S=7, B=2, C=len(class_name_to_idx),
    transform=transform
)
val_dataset = PillYoloDataset(
    image_dir=CONFIG_VAL,
    label_dir=CONFIG_VAL_LABEL,
    class_to_idx=class_name_to_idx,
    transform=transform
)




# class_txt_path = "YOLO_dataset/class_names.txt"
# existed_label_dir = "existed_label"
# new_label_dir = "data/new_anno"
# output_label_dir = "output_label"


#class_name_to_idx, _ = Utils.common.load_class_mapping(class_txt_path)

# === Run ===
# Utils.common.merge_labels_with_db(
#     CONFIG_TRAIN_LABEL,
#     new_label_dir,
#     output_label_dir,
#     class_name_to_idx,
#     DB_PATH
# )
#json_files = get_all_annotation_files(TRAIN_ANN_DIR)
#print(f"총 {len(json_files)}개의 annotation 파일을 찾았습니다.")
#df_annotations = analyze_pill_annotations(json_files)


#class_counts = plot_class_distribution(data, top_n=30)
#analyze_image_sizes(data)
#metadata_and_annotation_analysis(data)

# detect_missing_pills(
#     dataset=data,
#     train_img_dir=TRAIN_IMG_DIR,
#     output_dir=OUTPUT_IMG_DIR
# )



#check_annotation.py
import re
import os
import json
from collections import defaultdict

def extract_category_ids_from_filename(filename: str) -> list[int]:
    """Extract category IDs from a filename like K-aaaa-bbbb-cccc-dddd_x_y_z.png"""
    base = filename.split("_")[0]  # Remove trailing cam angle, size etc.
    parts = base.split("-")
    if len(parts) < 5:
        return []
    try:
        return [int(p) for p in parts[1:5]]
    except ValueError:
        return []


def find_missing_annotations(ann_dir, img_dir) -> dict:
    missing_annotations = defaultdict(list)

    for root, _, files in os.walk(ann_dir):
        for file in files:
            if not file.endswith(".json"):
                continue

            json_path = os.path.join(root, file)
            try:
                with open(json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            except Exception as e:
                print(f"[ERROR] Could not load {json_path}: {e}")
                continue

            # Extract actual category_ids from filename
            if not data.get("images") or not data.get("annotations"):
                continue

            imgfile = data["images"][0].get("imgfile")
            expected_cat_ids = extract_category_ids_from_filename(imgfile)

            # Extract actual category_ids from annotation
            actual_cat_ids = set()
            for ann in data["annotations"]:
                cat_id = ann.get("category_id")
                if cat_id is not None:
                    actual_cat_ids.add(cat_id)

            # Find missing category_ids
            for cat_id in expected_cat_ids:
                if cat_id not in actual_cat_ids:
                    missing_annotations[imgfile].append(cat_id)

    return missing_annotations