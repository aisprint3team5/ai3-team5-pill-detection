{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12517141,"sourceType":"datasetVersion","datasetId":7901002},{"sourceId":12521076,"sourceType":"datasetVersion","datasetId":7903579}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["import os\n","import shutil\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from pathlib import Path\n","import matplotlib.pyplot as plt\n","from PIL import Image, ImageDraw, ImageFont\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision.transforms as T\n","from torchvision.ops import nms\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import VOCDetection\n","from torch.utils.data import random_split\n","from torch.utils.data.dataloader import default_collate\n","from torch.optim.lr_scheduler import OneCycleLR"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T00:13:26.685997Z","iopub.execute_input":"2025-07-21T00:13:26.686557Z","iopub.status.idle":"2025-07-21T00:13:38.789831Z","shell.execute_reply.started":"2025-07-21T00:13:26.686523Z","shell.execute_reply":"2025-07-21T00:13:38.789050Z"},"id":"IAKFcoxHX1eJ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","src_dir = \"/kaggle/input/vocset\"\n","dst_dir = \"/kaggle/working/data\"\n","\n","# 1) 대상 디렉터리 생성 (이미 있으면 무시)\n","os.makedirs(dst_dir, exist_ok=True)\n","\n","# 2) src_dir의 모든 항목을 dst_dir로 복사\n","for entry in os.listdir(src_dir):\n","    src_path = os.path.join(src_dir, entry)\n","    dst_path = os.path.join(dst_dir, entry)\n","    if os.path.isdir(src_path):\n","        # 하위 폴더 통째로 복사\n","        shutil.copytree(src_path, dst_path, dirs_exist_ok=True)\n","    else:\n","        # 파일 단위로 복사\n","        shutil.copy2(src_path, dst_path)\n","\n","print(f\"Copied all from {src_dir} to {dst_dir}\")\n","\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-20T15:32:55.521882Z","iopub.execute_input":"2025-07-20T15:32:55.522124Z","iopub.status.idle":"2025-07-20T15:34:33.713073Z","shell.execute_reply.started":"2025-07-20T15:32:55.522100Z","shell.execute_reply":"2025-07-20T15:34:33.712394Z"},"id":"wqJvMpH8X1eN","outputId":"c451396c-f2d3-44f1-e379-ed47e11d8097"},"outputs":[{"name":"stdout","text":"Copied all from /kaggle/input/vocset to /kaggle/working/data\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":["## 1. Dataset 정의"],"metadata":{"id":"HXALfWBYX1eP"}},{"cell_type":"code","source":["\n","IS_DEV=False\n","\n","VOC_CLASSES = [\n","    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n","    'bus', 'car', 'cat', 'chair', 'cow',\n","    'diningtable', 'dog', 'horse', 'motorbike', 'person',\n","    'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n","]\n","class_to_idx = {cls_name: i for i, cls_name in enumerate(VOC_CLASSES)}\n","\n","\n","cfg = Config()\n","\n","# ------------------------------------------------------------------------------\n","# 0. Dataset & Dataloader 예시 (Pascal VOC)\n","# ------------------------------------------------------------------------------\n","def my_collate(batch):\n","    # batch = [(img1, tgt1, meta1), (img2, tgt2, meta2), ...]\n","    imgs, tgts, metas = zip(*batch)\n","\n","    # img, tgt는 모두 같은 shape이므로 default_collate 사용\n","    imgs = default_collate(imgs)    # → tensor (B, C, H, W)\n","    tgts = default_collate(tgts)    # → tensor (B, S, S, 5+C)\n","\n","    # metas는 dict마다 길이가 다르니 그냥 리스트로 넘겨줌\n","    metas = list(metas)             # → [meta1, meta2, ..., metaB]\n","\n","    return imgs, tgts, metas\n","\n","class VOCDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, year='2007', image_set='train', S=7, B=2, C=20, transform=None):\n","        self.dataset = VOCDetection(root, year=year, image_set=image_set, download=False)\n","        self.S, self.B, self.C = S, B, C\n","        self.transform = transform\n","        self.class_to_idx = class_to_idx\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        img, target = self.dataset[idx]\n","        boxes = []\n","        labels = []\n","        label_names = []\n","        for obj in target['annotation']['object']:\n","            bbox = obj['bndbox']\n","            # 원본 좌표 [1..W/H] → normalized [0..1]\n","            x1 = float(bbox['xmin']) / img.width\n","            y1 = float(bbox['ymin']) / img.height\n","            x2 = float(bbox['xmax']) / img.width\n","            y2 = float(bbox['ymax']) / img.height\n","            boxes.append([x1,y1,x2,y2])\n","            cls_name = obj['name']\n","            labels.append(self.class_to_idx[cls_name])\n","            label_names.append(cls_name)\n","\n","        if self.transform:\n","            img = self.transform(img)\n","\n","        # target tensor: [S, S, 5B + C], 초기 0\n","        target_tensor = torch.zeros((self.S, self.S, 5*self.B + self.C))\n","        cell_size = 1.0 / self.S\n","\n","        for box, cls in zip(boxes, labels):\n","            x1,y1,x2,y2 = box\n","            x_center = (x1 + x2) / 2\n","            y_center = (y1 + y2) / 2\n","            w = x2 - x1\n","            h = y2 - y1\n","\n","            i = int(y_center / cell_size)\n","            j = int(x_center / cell_size)\n","            # cell 내 상대 좌표\n","            dx = (x_center - j*cell_size) / cell_size\n","            dy = (y_center - i*cell_size) / cell_size\n","\n","            # 첫 번째 박스 책임 할당\n","            target_tensor[i,j,0:4] = torch.tensor([dx, dy, w, h])\n","            target_tensor[i,j,4] = 1\n","            target_tensor[i,j,5 * self.B + cls] = 1 # target_tensor[i,j,5+cls] = 1\n","\n","        meta = {\n","            'image_id': idx,\n","            'boxes': boxes,\n","            'labels': labels,\n","            'label_names': label_names,\n","        }\n","\n","        # print('target_tensor: ', target_tensor.shape)\n","\n","        return img, target_tensor, meta\n","\n","# transforms\n","transform = T.Compose([\n","    T.Resize((cfg.IMAGE_SIZE, cfg.IMAGE_SIZE)),\n","    # T.RandomHorizontalFlip(0.5),\n","    # T.ColorJitter(0.2, 0.2, 0.2, 0.1),\n","    T.ToTensor(),\n","    # T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n","])\n","# dataset & loader\n","train_ds = VOCDataset(root='./data', image_set='train', transform=transform, S=cfg.S, B=cfg.B, C=cfg.C)\n","val_ds   = VOCDataset(root='./data', image_set='val',   transform=transform, S=cfg.S, B=cfg.B, C=cfg.C)\n","if IS_DEV:\n","    #train_ds, _ = random_split(train_ds, [32, len(train_ds)-32])\n","    val_ds, _   = random_split(val_ds,   [32, len(val_ds)-32])\n","\n","    # 한개 이미지로 32개를 만들어서 오버핏 테스트\n","    from torch.utils.data import TensorDataset, Dataset\n","    class OverfitDataset(Dataset):\n","        \"\"\"\n","        동일한 (img, tgt, meta) 한 쌍을 batch_size만큼 복제해서\n","        __getitem__에서 (img, tgt, meta) 튜플을 반환하도록 합니다.\n","        \"\"\"\n","        def __init__(self, img0, tgt0, meta0, batch_size):\n","            # img0: (C,H,W) tensor, tgt0: (S,S,5+C) tensor, meta0: any picklable object\n","            self.imgs  = img0.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n","            self.tgts  = tgt0.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n","            # meta0가 튜플(예: (image_id, boxes, labels, label_names))이라면\n","            # 같은 객체를 batch_size번 참조해도 무방합니다.\n","            self.metas = [meta0 for _ in range(batch_size)]\n","\n","        def __len__(self):\n","            return len(self.imgs)\n","\n","        def __getitem__(self, idx):\n","            # idx번째에 대응하는 (img, tgt, meta) 튜플을 반환\n","            return self.imgs[idx], self.tgts[idx], self.metas[idx]\n","    img0, tgt0, meta0 = train_ds[1]\n","    train_ds   = OverfitDataset(img0, tgt0, meta0, 32)\n","    # 배치 차원으로 repeat\n","    # imgs_batch   = img0.unsqueeze(0).repeat(32, 1, 1, 1)      # (32, C, H, W)\n","    # tgts_batch   = tgt0.unsqueeze(0).repeat(32, 1, 1, 1)      # (32, S, S, 5+C)\n","\n","    # overfit_ldr  = DataLoader(overfit_ds, batch_size=32, shuffle=True)\n","\n","train_loader = DataLoader(train_ds, batch_size=cfg.BATCH_SIZE, shuffle=True, collate_fn=my_collate)\n","val_loader   = DataLoader(val_ds,   batch_size=cfg.BATCH_SIZE, collate_fn=my_collate)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T15:44:02.333233Z","iopub.execute_input":"2025-07-20T15:44:02.333502Z","iopub.status.idle":"2025-07-20T15:44:02.360799Z","shell.execute_reply.started":"2025-07-20T15:44:02.333482Z","shell.execute_reply":"2025-07-20T15:44:02.360145Z"},"id":"V821q2gSX1eS"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## 2. Util 함수 정의"],"metadata":{"id":"37xusezPX1eU"}},{"cell_type":"code","source":["class Utils:\n","    # ------------------------------------------------------------------------------\n","    # Post-processing: Decode + NMS\n","    # ------------------------------------------------------------------------------\n","    @staticmethod\n","    def postprocess(output, conf_thresh, iou_thresh, S, B, C, flatten=False):\n","        boxes, scores, classes = Utils.decode_predictions(output, conf_thresh, S, B, C)\n","        print('boxes: ', len(boxes), boxes[0].numel())\n","        print('scores: ', len(scores), scores[0].numel())\n","        print('classes: ', len(classes), classes[0].numel())\n","        detections = Utils.apply_nms(boxes, scores, classes, iou_thresh)\n","        print('detections: ', len(detections), detections[0].numel())\n","        if flatten:\n","            return torch.cat(detections, dim=0) # if flatten=True : 모든 이미지를 하나의 텐서 (∑K_i, 6)로 합침\n","        return detections # return: if flatten=False: 배치별 리스트 of 텐서 (K_i, 6) 반환\n","\n","    # 1) Decode 단계: 모델 출력 → 바운딩박스, 점수, 클래스 리스트로 변환\n","    @staticmethod\n","    def decode_predictions(output, conf_thresh, S, B, C):\n","        '''\n","        output: [N, S, S, 5B + C]\n","        returns:\n","        batch_boxes   : list of N tensors [M_i, 4]  (x1, y1, x2, y2)\n","        batch_scores  : list of N tensors [M_i]     (score)\n","        batch_classes : list of N tensors [M_i]     (class_idx)\n","        '''\n","        N, device = output.size(0), output.device\n","\n","        # 1) 셀 오프셋 계산 (한번만)\n","        grid_y, grid_x = torch.meshgrid(\n","            torch.arange(S, device=device),\n","            torch.arange(S, device=device),\n","            indexing='ij'\n","        )\n","        cell_offsets = torch.stack([grid_x, grid_y], dim=-1).unsqueeze(2)\n","        # shape = (S, S, 1, 2)\n","\n","        batch_boxes, batch_scores, batch_classes = [], [], []\n","        for b in range(N):\n","            single = output[b] # (S, S, 5*B + C)\n","\n","            # Model.forward()에서 이미 softmax 함 → 그대로 사용\n","            cls_probs = single[..., 5*B:]   # (S, S, C)\n","            # raw_boxes 에는 이미 x,y sigmoid, conf sigmoid\n","            raw_boxes = single[..., :5*B].view(S, S, B, 5)  # (S, S, B, [x,y,w,h,conf])\n","            # x,y,conf는 이미 활성화 끝 → 바로 쓰고\n","            xy   = raw_boxes[..., :2]        # (S, S, B, 2)\n","            conf = raw_boxes[..., 4]         # (S, S, B)\n","            # w,h는 raw → 논문대로 square\n","            wh   = raw_boxes[..., 2:4].pow(2) # (S, S, B, 2)\n","            # 두 박스 중 objectness(conf) 기준 책임박스 하나만\n","            best_conf, best_idx = conf.max(dim=-1)  # (S, S)\n","\n","            boxes, scores, classes = [], [], []\n","            for i in range(S):\n","                for j in range(S):\n","                    score_obj = best_conf[i,j].item()\n","                    if score_obj < conf_thresh:\n","                        continue\n","\n","                    bi = best_idx[i,j].item()  # 0 or 1\n","\n","                    # 2-1) 셀 오프셋 + 상대→절대 [0..1]\n","                    x_rel, y_rel = xy[i,j,bi]\n","                    x_center = (cell_offsets[i,j,0,0] + x_rel) / S\n","                    y_center = (cell_offsets[i,j,0,1] + y_rel) / S\n","\n","                    # 2-2) square→w,h\n","                    w_rel, h_rel = wh[i,j,bi]\n","\n","                    # 3) (cx,cy,w,h) → (x1,y1,x2,y2)\n","                    x1 = x_center - w_rel/2\n","                    y1 = y_center - h_rel/2\n","                    x2 = x_center + w_rel/2\n","                    y2 = y_center + h_rel/2\n","                    boxes.append([x1, y1, x2, y2]) # 이미지 전체 기준의 normalized 좌표(0…1)입니다.\n","\n","                    # 4) 클래스 점수 계산 (conf * cls_prob)\n","                    prob     = cls_probs[i,j]           # (C,)\n","                    cls_prob, cls_idx = prob.max(dim=-1)\n","                    scores.append((score_obj * cls_prob).item())\n","                    classes.append(cls_idx.item())\n","            if boxes:\n","                batch_boxes.append(torch.tensor(boxes, dtype=torch.float32).to(device))  # boxes는 List[List[Tensor]]\n","                batch_scores.append(torch.tensor(scores, dtype=torch.float32).to(device)) # scores는 List[Tensor]\n","                batch_classes.append(torch.tensor(classes, dtype=torch.long).to(device)) # classes도 List[Tensor]\n","            else:\n","                batch_boxes.append(torch.zeros((0,4)).to(device))\n","                batch_scores.append(torch.zeros((0,)).to(device))\n","                batch_classes.append(torch.zeros((0,), dtype=torch.long).to(device))\n","        return batch_boxes, batch_scores, batch_classes\n","\n","    # 2) NMS 단계: 클래스별로 Non-Maximum Suppression 적용\n","    @staticmethod\n","    def apply_nms(batch_boxes, batch_scores, batch_classes, iou_thresh):\n","        '''\n","        batch_boxes   : list of N tensors [M_i, 4]  (x1, y1, x2, y2)\n","        batch_scores  : list of N tensors [M_i]     (score)\n","        batch_classes : list of N tensors [M_i]     (class_idx)\n","        iou_thresh    : IoU 임계값 (e.g. 0.4)\n","        returns       : list of N tensors [K_i, 6]  (x1, y1, x2, y2, score, cls)\n","        '''\n","        batch_detections = []\n","\n","        for boxes, scores, classes in zip(batch_boxes, batch_scores, batch_classes):\n","            if boxes.numel() == 0: # numel: 텐서가 담고 있는 전체 원소 개수를 반환. (M, 4)면 M*4이고, (M, 0) or (0, 4)면 0이다.\n","                batch_detections.append(torch.zeros((0,6), dtype=boxes.dtype, device=boxes.device)) # [x1, y1, x2, y2, score, class_idx]\n","                continue\n","\n","            kept = [] # 한 이미지 내에서 클래스별 NMS를 거쳐 최종적으로 남은 박스 텐서들을 임시로 담아두는 파이썬 리스트\n","            for cls_id in classes.unique(): # 해당 이미지에서 예측된 클래스들(중복 제거)을 리스트로 얻음\n","                mask = (classes == cls_id) # (M,) 클래스별 박스만 선택\n","                cls_boxes  = boxes[mask]   # (m,4): (M, 4)는 아직 어떤 클래스 기준으로도 걸러내지 않은 상태. (m, 4)는 '지금 보고 있는 클래스'에 속하는 박스만 남긴 결과\n","                cls_scores = scores[mask]  # (m,): 클래스별 박스 수\n","\n","                # 예시\n","                # boxes = torch.tensor([\n","                #    [0,0,10,10],    # idx 0\n","                #    [1,1,11,11],    # idx 1\n","                #    [50,50,60,60],  # idx 2\n","                #    [70,70,80,80]   # idx 3\n","                # ])\n","                # scores = torch.tensor([0.9, 0.8, 0.7, 0.3])\n","                # keep = nms(boxes, scores, iou_thresh=0.4)\n","                # print(keep)  # tensor([0, 2, 3])\n","                # idx의 score가 0.8인데도 제거한 이유는 점수가 높은 1보다도 점수가 맞기 때문임.\n","                # idx 0와 idx 1의 IoU는 0.68로 iou_thresh 0.4 보다 높으므로 1이 제거됨\n","                # idx 3은 score가 0.3이지만 iou의 대상자체가 아니기에 출력에 포함. IoU는 겹칠때 중복된 상자를 삭제할때 쓰는 로직임\n","                keep_idxs  = nms(cls_boxes, cls_scores, iou_thresh) # 박스들의 index 리스트\n","\n","                if keep_idxs.numel() > 0:\n","                    cls_id_col = torch.full((keep_idxs.numel(),1), cls_id, dtype=boxes.dtype, device=boxes.device)\n","                    selected = torch.cat([\n","                        cls_boxes[keep_idxs], # (k, 4) -> k <= m\n","                        cls_scores[keep_idxs].unsqueeze(1), # (k, 1) -> k <= m\n","                        cls_id_col # (k, 1) -> k <= m\n","                    ], dim=1)  # dim이 1이므로 열방향으로 concat한다. (k,6)\n","                    kept.append(selected)\n","\n","            if kept:\n","                # if kept가 [torch.Size([2,6]), torch.Size([1,6]), torch.Size([4,6])] 면, batch_detections는 torch.Size([7,6])\n","                batch_detections.append(torch.vstack(kept)) # kept 안의 [k,6] 텐서를 이어 붙여 [K,6] 생성\n","            else:\n","                batch_detections.append(torch.zeros((0,6), dtype=boxes.dtype, device=boxes.device))\n","        return batch_detections\n","\n","def load_model(model, name):\n","    ckpt_file = Path(f'./models/{name}.pth')\n","    if ckpt_file.is_file():\n","        state = torch.load(ckpt_file, map_location=torch.device(cfg.DEVICE))\n","        model.load_state_dict(state)\n","        print(f\"Loaded checkpoint from {ckpt_file}\")\n","    else:\n","        print(f\"No checkpoint at {ckpt_file}, skipping load\")\n","        input_file = Path(f'/kaggle/input/modelpth/{name}.pth')\n","        if input_file.is_file():\n","            state = torch.load(input_file, map_location=torch.device(cfg.DEVICE))\n","            model.load_state_dict(state)\n","            print(f\"Loaded input from {input_file}\")\n","        else:\n","            print(f\"No loaded input at {ckpt_file}, skipping load\")\n","    def save_model():\n","        os.makedirs('./models', exist_ok=True)\n","        torch.save(model.state_dict(), ckpt_file)\n","    return save_model\n","\n","\n","def draw_detections_pil(image_tensor, detections, class_names, output_path=None):\n","    \"\"\"\n","    image_tensor : torch.Tensor (3, H, W), float [0,1]\n","    detections   : torch.Tensor or array (K,6) [x1,y1,x2,y2,score,cls_idx], normalized\n","    class_names  : 클래스 이름 리스트\n","    \"\"\"\n","    # 1) Tensor → H×W×3 uint8 → PIL\n","    img_np = (image_tensor\n","              .mul(255)\n","              .clamp(0,255)\n","              .byte()\n","              .permute(1,2,0)\n","              .cpu()\n","              .numpy())\n","    pil_img = Image.fromarray(img_np)\n","    draw = ImageDraw.Draw(pil_img)\n","\n","    # 2) 폰트 준비\n","    try:\n","        font = ImageFont.truetype(\"arial.ttf\", size=16)\n","    except IOError:\n","        font = ImageFont.load_default()\n","\n","    W, H = pil_img.size\n","    dets = detections.detach().cpu().tolist()\n","\n","    for x1,y1,x2,y2,score,cls_idx in dets:\n","        # 픽셀 좌표로 변환\n","        x1p, y1p = int(x1*W), int(y1*H)\n","        x2p, y2p = int(x2*W), int(y2*H)\n","\n","        # 3) 바운딩 박스\n","        draw.rectangle([x1p, y1p, x2p, y2p], outline=\"lime\", width=2)\n","\n","        # 4) 라벨 문자열\n","        label = f\"{class_names[int(cls_idx)]}:{score:.2f}\"\n","\n","        # 🚩 mask.size 로 텍스트 크기 구하기\n","        mask = font.getmask(label)\n","        tw, th = mask.size\n","\n","        # 5) 라벨 배경 그리기\n","        bg_xy = [x1p, y1p - th - 4, x1p + tw + 4, y1p]\n","        draw.rectangle(bg_xy, fill=\"black\")\n","\n","        # 6) 흰색 텍스트\n","        draw.text((x1p+2, y1p-th-2), label, font=font, fill=\"white\")\n","\n","    # 7) 저장 또는 Matplotlib 표시\n","    if output_path:\n","        pil_img.save(output_path)\n","    else:\n","        plt.figure(figsize=(8,6))\n","        plt.imshow(pil_img)\n","        plt.axis(\"off\")\n","        plt.show()\n","\n","    return pil_img\n","\n","\n","# 주어진 두 박스의 중심 좌표와 크기를 바탕으로 좌측상단, 우측하단 좌표를 계산하고, 교집합 영역을 통해 IoU를 계산합니다.\n","# IoU 계산 함수 (YOLOv1에서 사용하는 bounding box 형식: [x_center, y_center, width, height])\n","def iou_cxcywh(boxes1, boxes2, eps=1e-6):\n","    \"\"\"\n","    boxes1, boxes2: 텐서, 마지막 차원이 [x_center, y_center, width, height]\n","    \"\"\"\n","    # 좌측 상단, 우측 하단 좌표 계산. (x1,y1,x2,y2)로 변환\n","    box1_x1 = boxes1[:,0] - boxes1[:,2] / 2\n","    box1_y1 = boxes1[:,1] - boxes1[:,3] / 2\n","    box1_x2 = boxes1[:,0] + boxes1[:,2] / 2\n","    box1_y2 = boxes1[:,1] + boxes1[:,3] / 2\n","\n","    box2_x1 = boxes2[:,0] - boxes2[:,2] / 2\n","    box2_y1 = boxes2[:,1] - boxes2[:,3] / 2\n","    box2_x2 = boxes2[:,0] + boxes2[:,2] / 2\n","    box2_y2 = boxes2[:,1] + boxes2[:,3] / 2\n","\n","    # 교집합 영역\n","    x1 = torch.max(box1_x1, box2_x1)\n","    y1 = torch.max(box1_y1, box2_y1)\n","    x2 = torch.min(box1_x2, box2_x2)\n","    y2 = torch.min(box1_y2, box2_y2)\n","    inter_w  = (x2 - x1).clamp(min=0)\n","    inter_h  = (y2 - y1).clamp(min=0)\n","    inter = inter_w * inter_h\n","\n","    # 합집합 영역\n","    box1_area = torch.abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n","    box2_area = torch.abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n","    union = box1_area + box2_area - inter + eps\n","\n","    # IoU: Intersaction over Union\n","    iou_val = inter / union\n","    return iou_val\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T15:34:38.955239Z","iopub.execute_input":"2025-07-20T15:34:38.955698Z","iopub.status.idle":"2025-07-20T15:34:38.978216Z","shell.execute_reply.started":"2025-07-20T15:34:38.955669Z","shell.execute_reply":"2025-07-20T15:34:38.977458Z"},"jupyter":{"source_hidden":true},"id":"mwtV2WMNX1eU"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## 3. 모델 정의"],"metadata":{"id":"rc6A0ehgX1eW"}},{"cell_type":"code","source":["# ------------------------------------------------------------------------------\n","# 1. Model Definition (YOLOv1)\n","# ------------------------------------------------------------------------------\n","class YOLOv1(nn.Module):\n","    # 논문에 제시된 아키텍처 구성 (YOLOv1)\n","    architecture_config = [\n","        (7, 64, 2, 3),       # (kernel_size, filters, stride, padding)\n","        \"M\",                 # maxpool\n","        (3, 192, 1, 1),\n","        \"M\",\n","        (1, 128, 1, 0),\n","        (3, 256, 1, 1),\n","        (1, 256, 1, 0),\n","        (3, 512, 1, 1),\n","        \"M\",\n","        [(1, 256, 1, 0), (3, 512, 1, 1), 4],  # 해당 블록을 4번 반복\n","        (1, 512, 1, 0),\n","        (3, 1024, 1, 1),\n","        \"M\",\n","        [(1, 512, 1, 0), (3, 1024, 1, 1), 2],  # 해당 블록을 2번 반복\n","        (3, 1024, 1, 1),\n","        (3, 1024, 2, 1),\n","        (3, 1024, 1, 1),\n","        (3, 1024, 1, 1)\n","    ]\n","\n","    def __init__(self, in_channels=3, S=7, B=2, C=20, conf_thresh=0.2, iou_thresh=0.4): # split_size=7, num_boxes=2, num_classes=20\n","        super(YOLOv1, self).__init__()\n","        self.S, self.B, self.C = S, B, C\n","        self.conf_thresh, self.iou_thresh = conf_thresh, iou_thresh\n","        self.features = YOLOv1.create_conv_layers(self.architecture_config, in_channels)\n","        # 입력 이미지가 448x448인 경우, 마지막 컨볼루션 feature map은 7x7 (논문 기준)\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(1024 * 7 * 7, 4096),\n","            nn.LeakyReLU(0.1),\n","            nn.Dropout(0.5),  # 논문에서 사용한 dropout\n","            nn.Linear(4096, S * S * (C + B * 5))\n","        )\n","        self.loss_fn = YoloLoss(self.S, self.B, self.C)\n","\n","    def forward(self, x, targets=None):\n","        # x: [B,3,H,W] → features → [B,1024,S,S]\n","        x = self.features(x)\n","        # classifier → [B, S*S*(5B + C)]\n","        x = self.classifier(x)\n","        # reshape → [B, S, S, 5B + C]\n","        # print('YOLOv1 forward', x.view(-1, self.S, self.S, 5*self.B + self.C).size())\n","        # YOLOv1 forward torch.Size([16, 7, 7, 30])\n","        x = x.view(-1, self.S, self.S, 5*self.B + self.C) # 계산 복잡도를 낮추기 위해 (N, S, S, 5*B+C) 형태로 반환한다.\n","\n","        preds = x\n","\n","        # — 활성화 적용 —\n","        for b in range(self.B):\n","            off = 5*b\n","            # t_x, t_y → sigmoid\n","            preds[..., off:off+2]   = torch.sigmoid(preds[..., off:off+2])\n","            # confidence → sigmoid\n","            preds[..., off+4:off+5] = torch.sigmoid(preds[..., off+4:off+5])\n","\n","        # class logits → softmax\n","        preds[..., 5*self.B : ] = F.softmax(preds[..., 5*self.B : ], dim=-1)\n","        return preds\n","\n","        # if targets is not None:\n","        #     #perfect_preds = targets.clone()       # pred와 target 포맷이 동일해야 함\n","        #     #return self.loss_fn(perfect_preds, targets)\n","        #     return self.loss_fn(x, targets) # tensor scalar 값임\n","\n","        # # inference 모드\n","        # return YOLOv1.postprocess(x, self.conf_thresh, self.iou_thresh, self.S, self.B, self.C, flatten), x\n","\n","    @staticmethod\n","    def create_conv_layers(config, in_channels):\n","        layers = []\n","        for module in config:\n","            if type(module) == tuple:\n","                # 튜플 형태: (kernel_size, filters, stride, padding)\n","                kernel_size, filters, stride, padding = module\n","                layers.append(nn.Conv2d(in_channels, filters, kernel_size, stride, padding))\n","                layers.append(nn.LeakyReLU(0.1))\n","                in_channels = filters\n","            elif module == \"M\":\n","                layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n","            elif type(module) == list:\n","                # 리스트 형태: [ conv1 튜플, conv2 튜플, 반복 횟수 ]\n","                conv1, conv2, num_repeats = module\n","                for _ in range(num_repeats):\n","                    # 첫 번째 컨볼루션\n","                    k, f, s, p = conv1\n","                    layers.append(nn.Conv2d(in_channels, f, k, s, p))\n","                    layers.append(nn.LeakyReLU(0.1))\n","                    in_channels = f\n","                    # 두 번째 컨볼루션\n","                    k, f, s, p = conv2\n","                    layers.append(nn.Conv2d(in_channels, f, k, s, p))\n","                    layers.append(nn.LeakyReLU(0.1))\n","                    in_channels = f\n","        return nn.Sequential(*layers)\n","\n","# ────────────────────────────────────────────────────────────────\n","# 2. YOLOv1 손실 함수 (채널 매핑: 0–4 box1,4 conf1,5–9 box2,9 conf2,10–29 class)\n","# ────────────────────────────────────────────────────────────────\n","def YoloLoss(S, B, C, lambda_coord=5.0, lambda_noobj=0.5):\n","    def iou_xyxy(boxes1, boxes2):\n","        x11, y11, x12, y12 = boxes1.unbind(-1)\n","        x21, y21, x22, y22 = boxes2.unbind(-1)\n","\n","        inter_x1 = torch.max(x11, x21)\n","        inter_y1 = torch.max(y11, y21)\n","        inter_x2 = torch.min(x12, x22)\n","        inter_y2 = torch.min(y12, y22)\n","\n","        inter_w = (inter_x2 - inter_x1).clamp(min=0)\n","        inter_h = (inter_y2 - inter_y1).clamp(min=0)\n","        inter_area = inter_w * inter_h\n","\n","        area1 = (x12 - x11) * (y12 - y11)\n","        area2 = (x22 - x21) * (y22 - y21)\n","        union = area1 + area2 - inter_area\n","        return inter_area / union.clamp(min=1e-6)\n","\n","    def xywh_to_xyxy(box):\n","        cx, cy, w, h = box.unbind(-1)\n","        x1 = cx - w/2\n","        y1 = cy - h/2\n","        x2 = cx + w/2\n","        y2 = cy + h/2\n","        return torch.stack([x1, y1, x2, y2], dim=-1)\n","\n","    def yolo_loss(predictions, target):\n","                  # S=7, B=2, C=20,\n","                  #lambda_coord=5.0, lambda_noobj=0.5):\n","        \"\"\"\n","        predictions: (batch, S*S*(5*B + C))\n","        target:      (batch, S, S, 5*B + C)\n","        \"\"\"\n","        device = predictions.device\n","\n","        # 1) reshape to (batch, S, S, 5*B + C)\n","        preds = predictions.view(-1, S, S, 5*B + C)\n","\n","        # 2) 그리드 오프셋 생성\n","        grid_y, grid_x = torch.meshgrid(\n","            torch.arange(S, device=device),\n","            torch.arange(S, device=device),\n","            indexing='ij'\n","        )\n","        grid = torch.stack([grid_x, grid_y], dim=-1).view(1, S, S, 1, 2)\n","\n","        # 3) 두 박스 슬롯 (cx,cy,w,h) 추출\n","        rel1 = preds[...,  0:4].view(-1, S, S, 1, 4)\n","        rel2 = preds[...,  5:9].view(-1, S, S, 1, 4)\n","\n","        # 4) 셀 내 상대 → 절대좌표 [0..1] 디코딩\n","        abs1 = torch.cat([\n","            (grid + rel1[..., :2]) / S,    # center\n","            rel1[..., 2:4]                 # w,h\n","        ], dim=-1)  # (batch, S, S, 1, 4)\n","\n","        abs2 = torch.cat([\n","            (grid + rel2[..., :2]) / S,\n","            rel2[..., 2:4]\n","        ], dim=-1)\n","\n","        # 5) GT 박스도 동일하게 디코딩 (target[...,0:4])\n","        gt_rel = target[..., 0:4].view(-1, S, S, 1, 4)\n","        gt_abs = torch.cat([\n","            (grid + gt_rel[..., :2]) / S,\n","            gt_rel[..., 2:4]\n","        ], dim=-1)\n","\n","        bb1 = xywh_to_xyxy(abs1).squeeze(3)  # (batch, S, S, 4)\n","        bb2 = xywh_to_xyxy(abs2).squeeze(3)\n","        gt  = xywh_to_xyxy(gt_abs).squeeze(3)\n","\n","        # 6) IoU 계산 & 책임 박스 선정\n","        iou_b1 = iou_xyxy(bb1, gt)\n","        iou_b2 = iou_xyxy(bb2, gt)\n","        ious   = torch.stack([iou_b1, iou_b2], dim=0)       # (2, batch, S, S)\n","        iou_maxes, bestbox = torch.max(ious, dim=0)         # (batch,S,S)\n","        bestbox = bestbox.unsqueeze(-1).float()             # (batch,S,S,1)\n","\n","        # 7) object mask (conf1 자리: channel 4)\n","        exists_box = target[..., 4].unsqueeze(-1)           # (batch,S,S,1)\n","\n","        # 8) Box 좌표 손실\n","        box_pred = exists_box * (\n","            bestbox * preds[..., 5:9] +      # slot2 coords\n","            (1-bestbox) * preds[..., 0:4]    # slot1 coords\n","        )\n","        box_tgt  = exists_box * target[...,  0:4]\n","\n","        # sqrt(w), sqrt(h)\n","        p_wh = torch.sqrt(box_pred[..., 2:4].clamp(min=1e-6))\n","        t_wh = torch.sqrt(box_tgt[...,  2:4].clamp(min=1e-6))\n","        box_predxy  = torch.cat([box_pred[..., :2], p_wh], dim=-1)\n","        box_target  = torch.cat([box_tgt[...,  :2], t_wh], dim=-1)\n","\n","        box_loss = torch.sum((box_predxy - box_target) ** 2)\n","\n","        # 9) Object confidence loss\n","        pred_conf   = bestbox * preds[..., 9:10] + (1-bestbox) * preds[..., 4:5]\n","        conf_target = exists_box * iou_maxes.unsqueeze(-1)\n","        obj_conf_loss = torch.sum((exists_box * (pred_conf - conf_target)) ** 2)\n","\n","        # 10) No-object confidence loss\n","        noobj_mask  = 1 - exists_box\n","        noobj_loss  = torch.sum((noobj_mask * preds[..., 4:5]) ** 2)\n","        noobj_loss += torch.sum((noobj_mask * preds[..., 9:10]) ** 2)\n","\n","        # 11) Class probability loss (channels 10~10+C)\n","        cls_pred   = preds[..., 10:10+C]\n","        cls_target = target[...,10:10+C]\n","        class_loss = torch.sum((exists_box * (cls_pred - cls_target)) ** 2)\n","\n","        # 12) 총합\n","        total_loss = (\n","            lambda_coord * box_loss +\n","            obj_conf_loss +\n","            lambda_noobj * noobj_loss +\n","            class_loss\n","        )\n","        batch_size = predictions.size(0)\n","        return total_loss / batch_size # 수정1: 배치 사이즈로 나눠준다. original: return total_loss\n","    return yolo_loss\n","\n","\n","# ------------------------------------------------------------------------------\n","# 3. Training & Validation Loop\n","# ------------------------------------------------------------------------------\n","def train_one_epoch(model, loader, opt, scheduler, device):\n","    model.train()\n","    total_loss = 0\n","    for imgs, targets, metas in tqdm(loader, desc='Train batchs'):\n","        imgs, targets = imgs.to(device), targets.to(device)\n","        #loss = model(imgs, targets)\n","        preds = model(imgs)\n","        loss = model.loss_fn(preds, targets)\n","        # print('train matas', metas)\n","        # print('train_one_epoch', preds.shape)\n","        # loss = loss_fn(preds, targets)\n","        opt.zero_grad()\n","        loss.backward()\n","        opt.step()\n","        # scheduler.step()\n","        total_loss += loss.item()\n","    return total_loss / len(loader)\n","\n","def validate(model, loader, device):\n","    model.eval()\n","    total_loss = 0\n","    with torch.no_grad():\n","        for imgs, targets, metas in loader:\n","            imgs, targets = imgs.to(device), targets.to(device)\n","            #loss = model(imgs, targets)\n","            preds = model(imgs)\n","            loss = model.loss_fn(preds, targets)\n","\n","            # print('val matas', metas)\n","            # print('loss: ', loss)\n","            # loss = loss_fn(preds, targets)\n","            total_loss += loss.item()\n","    return total_loss / len(loader)"],"metadata":{"trusted":true,"id":"fUEpRr89X1eW"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## 4. 훈련 및 추론"],"metadata":{"id":"Yu0lH1i9X1eY"}},{"cell_type":"code","source":["# ------------------------------------------------------------------------------\n","# 4. Main: 학습 및 추론 예시\n","# ------------------------------------------------------------------------------\n","\n","# 모델·손실·최적화기\n","model = YOLOv1(cfg.IMAGE_CH_SIZE, cfg.S, cfg.B, cfg.C, cfg.CONF_THRESHOLD, cfg.NMS_IOU_THRESH).to(cfg.DEVICE)\n","save_model = load_model(model, 'model3')\n","#opt = optim.Adam(model.parameters(), lr=cfg.LR) #, weight_decay=cfg.WD\n","opt = torch.optim.SGD(model.parameters(), lr=cfg.LR, momentum=0.9, weight_decay=cfg.WD)\n","total_steps = cfg.EPOCHS * len(train_loader)\n","scheduler = OneCycleLR(\n","    opt,\n","    max_lr=1e-2,\n","    total_steps=total_steps,\n","    pct_start=0.1,  # warmup 비율\n","    anneal_strategy='cos'\n",")\n","\n","# 학습 루프\n","for epoch in range(1, cfg.EPOCHS+1):\n","    train_loss = train_one_epoch(model, train_loader, opt, scheduler, cfg.DEVICE)\n","    val_loss   = validate(model, val_loader, cfg.DEVICE)\n","    print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n","    if epoch % 5 == 0:\n","        save_model()\n","\n","# 추론 예시\n","model.eval()\n","with torch.no_grad():\n","    test_img, _, meta = val_ds[0]\n","    detections = model(test_img.unsqueeze(0).to(cfg.DEVICE))\n","    print(\"Sample detections:\", detections[0])\n","    print(\"meta:\", meta)\n","\n","'''\n","Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.29it/s]\n","Epoch 01 | Train Loss: 7.2043 | Val Loss: 6.7486\n","Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.31it/s]\n","Epoch 02 | Train Loss: 7.0637 | Val Loss: 6.5826\n","Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.32it/s]\n","Epoch 03 | Train Loss: 6.7607 | Val Loss: 6.4628\n","Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.31it/s]\n","Epoch 04 | Train Loss: 6.7036 | Val Loss: 6.5068\n","Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.30it/s]\n","Epoch 05 | Train Loss: 6.7157 | Val Loss: 6.3505\n","Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.28it/s]\n","Epoch 06 | Train Loss: 6.7842 | Val Loss: 6.4915\n","Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.31it/s]\n","Epoch 07 | Train Loss: 6.6331 | Val Loss: 6.4501\n","Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.32it/s]\n","Epoch 08 | Train Loss: 6.6444 | Val Loss: 6.2640\n","Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.31it/s]\n","Epoch 09 | Train Loss: 6.5844 | Val Loss: 6.2967\n","Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.31it/s]\n","Epoch 10 | Train Loss: 6.5646 | Val Loss: 6.3164\n","Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.31it/s]\n","Epoch 11 | Train Loss: 6.5353 | Val Loss: 6.2102\n","Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.31it/s]\n","Epoch 12 | Train Loss: 6.9539 | Val Loss: 7.7272\n","Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.28it/s]\n","'''\n","print()\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T15:44:27.647038Z","iopub.execute_input":"2025-07-20T15:44:27.647332Z","execution_failed":"2025-07-20T23:25:19.072Z"},"id":"XCDl8VPwX1eZ","outputId":"3f5d5d42-2173-4b06-8ad9-f2d89196f494"},"outputs":[{"name":"stdout","text":"Loaded checkpoint from models/model3.pth\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | Train Loss: 7.2043 | Val Loss: 6.7486\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 02 | Train Loss: 7.0637 | Val Loss: 6.5826\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 03 | Train Loss: 6.7607 | Val Loss: 6.4628\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 04 | Train Loss: 6.7036 | Val Loss: 6.5068\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 05 | Train Loss: 6.7157 | Val Loss: 6.3505\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 06 | Train Loss: 6.7842 | Val Loss: 6.4915\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 07 | Train Loss: 6.6331 | Val Loss: 6.4501\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 08 | Train Loss: 6.6444 | Val Loss: 6.2640\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 09 | Train Loss: 6.5844 | Val Loss: 6.2967\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss: 6.5646 | Val Loss: 6.3164\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | Train Loss: 6.5353 | Val Loss: 6.2102\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | Train Loss: 6.9539 | Val Loss: 7.7272\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|██████████| 157/157 [00:47<00:00,  3.28it/s]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["### Inference 로직 ###\n","\n","model = YOLOv1(cfg.IMAGE_CH_SIZE, cfg.S, cfg.B, cfg.C, cfg.CONF_THRESHOLD, cfg.NMS_IOU_THRESH).to(cfg.DEVICE)\n","save_model = load_model(model, 'model1')\n","\n","# 추론 예시\n","model.eval()\n","with torch.no_grad():\n","    for i in range(10): # len(val_ds)\n","        test_img, _, meta = val_ds[i] # train_ds[i] #val_ds[i]\n","        print(test_img.size())\n","        preds = model(test_img.unsqueeze(0).to(cfg.DEVICE))\n","        detections = Utils.postprocess(preds, cfg.CONF_THRESHOLD, cfg.NMS_IOU_THRESH, cfg.S, cfg.B, cfg.C)\n","        draw_detections_pil(test_img, detections[0], VOC_CLASSES)\n","        print(\"Sample detections:\", detections[0])\n","        print('meta: ', meta)"],"metadata":{"trusted":true,"id":"jKwUYf-gX1ea"},"outputs":[],"execution_count":null}]}