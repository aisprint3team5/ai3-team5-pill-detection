{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12517141,"sourceType":"datasetVersion","datasetId":7901002},{"sourceId":12521076,"sourceType":"datasetVersion","datasetId":7903579}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["import os\n","import shutil\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from pathlib import Path\n","import matplotlib.pyplot as plt\n","from PIL import Image, ImageDraw, ImageFont\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision.transforms as T\n","from torchvision.ops import nms\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import VOCDetection\n","from torch.utils.data import random_split\n","from torch.utils.data.dataloader import default_collate\n","from torch.optim.lr_scheduler import OneCycleLR"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T00:13:26.685997Z","iopub.execute_input":"2025-07-21T00:13:26.686557Z","iopub.status.idle":"2025-07-21T00:13:38.789831Z","shell.execute_reply.started":"2025-07-21T00:13:26.686523Z","shell.execute_reply":"2025-07-21T00:13:38.789050Z"},"id":"IAKFcoxHX1eJ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","src_dir = \"/kaggle/input/vocset\"\n","dst_dir = \"/kaggle/working/data\"\n","\n","# 1) ëŒ€ìƒ ë””ë ‰í„°ë¦¬ ìƒì„± (ì´ë¯¸ ìˆìœ¼ë©´ ë¬´ì‹œ)\n","os.makedirs(dst_dir, exist_ok=True)\n","\n","# 2) src_dirì˜ ëª¨ë“  í•­ëª©ì„ dst_dirë¡œ ë³µì‚¬\n","for entry in os.listdir(src_dir):\n","    src_path = os.path.join(src_dir, entry)\n","    dst_path = os.path.join(dst_dir, entry)\n","    if os.path.isdir(src_path):\n","        # í•˜ìœ„ í´ë” í†µì§¸ë¡œ ë³µì‚¬\n","        shutil.copytree(src_path, dst_path, dirs_exist_ok=True)\n","    else:\n","        # íŒŒì¼ ë‹¨ìœ„ë¡œ ë³µì‚¬\n","        shutil.copy2(src_path, dst_path)\n","\n","print(f\"Copied all from {src_dir} to {dst_dir}\")\n","\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-20T15:32:55.521882Z","iopub.execute_input":"2025-07-20T15:32:55.522124Z","iopub.status.idle":"2025-07-20T15:34:33.713073Z","shell.execute_reply.started":"2025-07-20T15:32:55.522100Z","shell.execute_reply":"2025-07-20T15:34:33.712394Z"},"id":"wqJvMpH8X1eN","outputId":"c451396c-f2d3-44f1-e379-ed47e11d8097"},"outputs":[{"name":"stdout","text":"Copied all from /kaggle/input/vocset to /kaggle/working/data\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":["## 1. Dataset ì •ì˜"],"metadata":{"id":"HXALfWBYX1eP"}},{"cell_type":"code","source":["\n","IS_DEV=False\n","\n","VOC_CLASSES = [\n","    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n","    'bus', 'car', 'cat', 'chair', 'cow',\n","    'diningtable', 'dog', 'horse', 'motorbike', 'person',\n","    'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n","]\n","class_to_idx = {cls_name: i for i, cls_name in enumerate(VOC_CLASSES)}\n","\n","\n","cfg = Config()\n","\n","# ------------------------------------------------------------------------------\n","# 0. Dataset & Dataloader ì˜ˆì‹œ (Pascal VOC)\n","# ------------------------------------------------------------------------------\n","def my_collate(batch):\n","    # batch = [(img1, tgt1, meta1), (img2, tgt2, meta2), ...]\n","    imgs, tgts, metas = zip(*batch)\n","\n","    # img, tgtëŠ” ëª¨ë‘ ê°™ì€ shapeì´ë¯€ë¡œ default_collate ì‚¬ìš©\n","    imgs = default_collate(imgs)    # â†’ tensor (B, C, H, W)\n","    tgts = default_collate(tgts)    # â†’ tensor (B, S, S, 5+C)\n","\n","    # metasëŠ” dictë§ˆë‹¤ ê¸¸ì´ê°€ ë‹¤ë¥´ë‹ˆ ê·¸ëƒ¥ ë¦¬ìŠ¤íŠ¸ë¡œ ë„˜ê²¨ì¤Œ\n","    metas = list(metas)             # â†’ [meta1, meta2, ..., metaB]\n","\n","    return imgs, tgts, metas\n","\n","class VOCDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, year='2007', image_set='train', S=7, B=2, C=20, transform=None):\n","        self.dataset = VOCDetection(root, year=year, image_set=image_set, download=False)\n","        self.S, self.B, self.C = S, B, C\n","        self.transform = transform\n","        self.class_to_idx = class_to_idx\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        img, target = self.dataset[idx]\n","        boxes = []\n","        labels = []\n","        label_names = []\n","        for obj in target['annotation']['object']:\n","            bbox = obj['bndbox']\n","            # ì›ë³¸ ì¢Œí‘œ [1..W/H] â†’ normalized [0..1]\n","            x1 = float(bbox['xmin']) / img.width\n","            y1 = float(bbox['ymin']) / img.height\n","            x2 = float(bbox['xmax']) / img.width\n","            y2 = float(bbox['ymax']) / img.height\n","            boxes.append([x1,y1,x2,y2])\n","            cls_name = obj['name']\n","            labels.append(self.class_to_idx[cls_name])\n","            label_names.append(cls_name)\n","\n","        if self.transform:\n","            img = self.transform(img)\n","\n","        # target tensor: [S, S, 5B + C], ì´ˆê¸° 0\n","        target_tensor = torch.zeros((self.S, self.S, 5*self.B + self.C))\n","        cell_size = 1.0 / self.S\n","\n","        for box, cls in zip(boxes, labels):\n","            x1,y1,x2,y2 = box\n","            x_center = (x1 + x2) / 2\n","            y_center = (y1 + y2) / 2\n","            w = x2 - x1\n","            h = y2 - y1\n","\n","            i = int(y_center / cell_size)\n","            j = int(x_center / cell_size)\n","            # cell ë‚´ ìƒëŒ€ ì¢Œí‘œ\n","            dx = (x_center - j*cell_size) / cell_size\n","            dy = (y_center - i*cell_size) / cell_size\n","\n","            # ì²« ë²ˆì§¸ ë°•ìŠ¤ ì±…ì„ í• ë‹¹\n","            target_tensor[i,j,0:4] = torch.tensor([dx, dy, w, h])\n","            target_tensor[i,j,4] = 1\n","            target_tensor[i,j,5 * self.B + cls] = 1 # target_tensor[i,j,5+cls] = 1\n","\n","        meta = {\n","            'image_id': idx,\n","            'boxes': boxes,\n","            'labels': labels,\n","            'label_names': label_names,\n","        }\n","\n","        # print('target_tensor: ', target_tensor.shape)\n","\n","        return img, target_tensor, meta\n","\n","# transforms\n","transform = T.Compose([\n","    T.Resize((cfg.IMAGE_SIZE, cfg.IMAGE_SIZE)),\n","    # T.RandomHorizontalFlip(0.5),\n","    # T.ColorJitter(0.2, 0.2, 0.2, 0.1),\n","    T.ToTensor(),\n","    # T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n","])\n","# dataset & loader\n","train_ds = VOCDataset(root='./data', image_set='train', transform=transform, S=cfg.S, B=cfg.B, C=cfg.C)\n","val_ds   = VOCDataset(root='./data', image_set='val',   transform=transform, S=cfg.S, B=cfg.B, C=cfg.C)\n","if IS_DEV:\n","    #train_ds, _ = random_split(train_ds, [32, len(train_ds)-32])\n","    val_ds, _   = random_split(val_ds,   [32, len(val_ds)-32])\n","\n","    # í•œê°œ ì´ë¯¸ì§€ë¡œ 32ê°œë¥¼ ë§Œë“¤ì–´ì„œ ì˜¤ë²„í• í…ŒìŠ¤íŠ¸\n","    from torch.utils.data import TensorDataset, Dataset\n","    class OverfitDataset(Dataset):\n","        \"\"\"\n","        ë™ì¼í•œ (img, tgt, meta) í•œ ìŒì„ batch_sizeë§Œí¼ ë³µì œí•´ì„œ\n","        __getitem__ì—ì„œ (img, tgt, meta) íŠœí”Œì„ ë°˜í™˜í•˜ë„ë¡ í•©ë‹ˆë‹¤.\n","        \"\"\"\n","        def __init__(self, img0, tgt0, meta0, batch_size):\n","            # img0: (C,H,W) tensor, tgt0: (S,S,5+C) tensor, meta0: any picklable object\n","            self.imgs  = img0.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n","            self.tgts  = tgt0.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n","            # meta0ê°€ íŠœí”Œ(ì˜ˆ: (image_id, boxes, labels, label_names))ì´ë¼ë©´\n","            # ê°™ì€ ê°ì²´ë¥¼ batch_sizeë²ˆ ì°¸ì¡°í•´ë„ ë¬´ë°©í•©ë‹ˆë‹¤.\n","            self.metas = [meta0 for _ in range(batch_size)]\n","\n","        def __len__(self):\n","            return len(self.imgs)\n","\n","        def __getitem__(self, idx):\n","            # idxë²ˆì§¸ì— ëŒ€ì‘í•˜ëŠ” (img, tgt, meta) íŠœí”Œì„ ë°˜í™˜\n","            return self.imgs[idx], self.tgts[idx], self.metas[idx]\n","    img0, tgt0, meta0 = train_ds[1]\n","    train_ds   = OverfitDataset(img0, tgt0, meta0, 32)\n","    # ë°°ì¹˜ ì°¨ì›ìœ¼ë¡œ repeat\n","    # imgs_batch   = img0.unsqueeze(0).repeat(32, 1, 1, 1)      # (32, C, H, W)\n","    # tgts_batch   = tgt0.unsqueeze(0).repeat(32, 1, 1, 1)      # (32, S, S, 5+C)\n","\n","    # overfit_ldr  = DataLoader(overfit_ds, batch_size=32, shuffle=True)\n","\n","train_loader = DataLoader(train_ds, batch_size=cfg.BATCH_SIZE, shuffle=True, collate_fn=my_collate)\n","val_loader   = DataLoader(val_ds,   batch_size=cfg.BATCH_SIZE, collate_fn=my_collate)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T15:44:02.333233Z","iopub.execute_input":"2025-07-20T15:44:02.333502Z","iopub.status.idle":"2025-07-20T15:44:02.360799Z","shell.execute_reply.started":"2025-07-20T15:44:02.333482Z","shell.execute_reply":"2025-07-20T15:44:02.360145Z"},"id":"V821q2gSX1eS"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## 2. Util í•¨ìˆ˜ ì •ì˜"],"metadata":{"id":"37xusezPX1eU"}},{"cell_type":"code","source":["class Utils:\n","    # ------------------------------------------------------------------------------\n","    # Post-processing: Decode + NMS\n","    # ------------------------------------------------------------------------------\n","    @staticmethod\n","    def postprocess(output, conf_thresh, iou_thresh, S, B, C, flatten=False):\n","        boxes, scores, classes = Utils.decode_predictions(output, conf_thresh, S, B, C)\n","        print('boxes: ', len(boxes), boxes[0].numel())\n","        print('scores: ', len(scores), scores[0].numel())\n","        print('classes: ', len(classes), classes[0].numel())\n","        detections = Utils.apply_nms(boxes, scores, classes, iou_thresh)\n","        print('detections: ', len(detections), detections[0].numel())\n","        if flatten:\n","            return torch.cat(detections, dim=0) # if flatten=True : ëª¨ë“  ì´ë¯¸ì§€ë¥¼ í•˜ë‚˜ì˜ í…ì„œ (âˆ‘K_i, 6)ë¡œ í•©ì¹¨\n","        return detections # return: if flatten=False: ë°°ì¹˜ë³„ ë¦¬ìŠ¤íŠ¸ of í…ì„œ (K_i, 6) ë°˜í™˜\n","\n","    # 1) Decode ë‹¨ê³„: ëª¨ë¸ ì¶œë ¥ â†’ ë°”ìš´ë”©ë°•ìŠ¤, ì ìˆ˜, í´ë˜ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n","    @staticmethod\n","    def decode_predictions(output, conf_thresh, S, B, C):\n","        '''\n","        output: [N, S, S, 5B + C]\n","        returns:\n","        batch_boxes   : list of N tensors [M_i, 4]  (x1, y1, x2, y2)\n","        batch_scores  : list of N tensors [M_i]     (score)\n","        batch_classes : list of N tensors [M_i]     (class_idx)\n","        '''\n","        N, device = output.size(0), output.device\n","\n","        # 1) ì…€ ì˜¤í”„ì…‹ ê³„ì‚° (í•œë²ˆë§Œ)\n","        grid_y, grid_x = torch.meshgrid(\n","            torch.arange(S, device=device),\n","            torch.arange(S, device=device),\n","            indexing='ij'\n","        )\n","        cell_offsets = torch.stack([grid_x, grid_y], dim=-1).unsqueeze(2)\n","        # shape = (S, S, 1, 2)\n","\n","        batch_boxes, batch_scores, batch_classes = [], [], []\n","        for b in range(N):\n","            single = output[b] # (S, S, 5*B + C)\n","\n","            # Model.forward()ì—ì„œ ì´ë¯¸ softmax í•¨ â†’ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n","            cls_probs = single[..., 5*B:]   # (S, S, C)\n","            # raw_boxes ì—ëŠ” ì´ë¯¸ x,y sigmoid, conf sigmoid\n","            raw_boxes = single[..., :5*B].view(S, S, B, 5)  # (S, S, B, [x,y,w,h,conf])\n","            # x,y,confëŠ” ì´ë¯¸ í™œì„±í™” ë â†’ ë°”ë¡œ ì“°ê³ \n","            xy   = raw_boxes[..., :2]        # (S, S, B, 2)\n","            conf = raw_boxes[..., 4]         # (S, S, B)\n","            # w,hëŠ” raw â†’ ë…¼ë¬¸ëŒ€ë¡œ square\n","            wh   = raw_boxes[..., 2:4].pow(2) # (S, S, B, 2)\n","            # ë‘ ë°•ìŠ¤ ì¤‘ objectness(conf) ê¸°ì¤€ ì±…ì„ë°•ìŠ¤ í•˜ë‚˜ë§Œ\n","            best_conf, best_idx = conf.max(dim=-1)  # (S, S)\n","\n","            boxes, scores, classes = [], [], []\n","            for i in range(S):\n","                for j in range(S):\n","                    score_obj = best_conf[i,j].item()\n","                    if score_obj < conf_thresh:\n","                        continue\n","\n","                    bi = best_idx[i,j].item()  # 0 or 1\n","\n","                    # 2-1) ì…€ ì˜¤í”„ì…‹ + ìƒëŒ€â†’ì ˆëŒ€ [0..1]\n","                    x_rel, y_rel = xy[i,j,bi]\n","                    x_center = (cell_offsets[i,j,0,0] + x_rel) / S\n","                    y_center = (cell_offsets[i,j,0,1] + y_rel) / S\n","\n","                    # 2-2) squareâ†’w,h\n","                    w_rel, h_rel = wh[i,j,bi]\n","\n","                    # 3) (cx,cy,w,h) â†’ (x1,y1,x2,y2)\n","                    x1 = x_center - w_rel/2\n","                    y1 = y_center - h_rel/2\n","                    x2 = x_center + w_rel/2\n","                    y2 = y_center + h_rel/2\n","                    boxes.append([x1, y1, x2, y2]) # ì´ë¯¸ì§€ ì „ì²´ ê¸°ì¤€ì˜ normalized ì¢Œí‘œ(0â€¦1)ì…ë‹ˆë‹¤.\n","\n","                    # 4) í´ë˜ìŠ¤ ì ìˆ˜ ê³„ì‚° (conf * cls_prob)\n","                    prob     = cls_probs[i,j]           # (C,)\n","                    cls_prob, cls_idx = prob.max(dim=-1)\n","                    scores.append((score_obj * cls_prob).item())\n","                    classes.append(cls_idx.item())\n","            if boxes:\n","                batch_boxes.append(torch.tensor(boxes, dtype=torch.float32).to(device))  # boxesëŠ” List[List[Tensor]]\n","                batch_scores.append(torch.tensor(scores, dtype=torch.float32).to(device)) # scoresëŠ” List[Tensor]\n","                batch_classes.append(torch.tensor(classes, dtype=torch.long).to(device)) # classesë„ List[Tensor]\n","            else:\n","                batch_boxes.append(torch.zeros((0,4)).to(device))\n","                batch_scores.append(torch.zeros((0,)).to(device))\n","                batch_classes.append(torch.zeros((0,), dtype=torch.long).to(device))\n","        return batch_boxes, batch_scores, batch_classes\n","\n","    # 2) NMS ë‹¨ê³„: í´ë˜ìŠ¤ë³„ë¡œ Non-Maximum Suppression ì ìš©\n","    @staticmethod\n","    def apply_nms(batch_boxes, batch_scores, batch_classes, iou_thresh):\n","        '''\n","        batch_boxes   : list of N tensors [M_i, 4]  (x1, y1, x2, y2)\n","        batch_scores  : list of N tensors [M_i]     (score)\n","        batch_classes : list of N tensors [M_i]     (class_idx)\n","        iou_thresh    : IoU ì„ê³„ê°’ (e.g. 0.4)\n","        returns       : list of N tensors [K_i, 6]  (x1, y1, x2, y2, score, cls)\n","        '''\n","        batch_detections = []\n","\n","        for boxes, scores, classes in zip(batch_boxes, batch_scores, batch_classes):\n","            if boxes.numel() == 0: # numel: í…ì„œê°€ ë‹´ê³  ìˆëŠ” ì „ì²´ ì›ì†Œ ê°œìˆ˜ë¥¼ ë°˜í™˜. (M, 4)ë©´ M*4ì´ê³ , (M, 0) or (0, 4)ë©´ 0ì´ë‹¤.\n","                batch_detections.append(torch.zeros((0,6), dtype=boxes.dtype, device=boxes.device)) # [x1, y1, x2, y2, score, class_idx]\n","                continue\n","\n","            kept = [] # í•œ ì´ë¯¸ì§€ ë‚´ì—ì„œ í´ë˜ìŠ¤ë³„ NMSë¥¼ ê±°ì³ ìµœì¢…ì ìœ¼ë¡œ ë‚¨ì€ ë°•ìŠ¤ í…ì„œë“¤ì„ ì„ì‹œë¡œ ë‹´ì•„ë‘ëŠ” íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸\n","            for cls_id in classes.unique(): # í•´ë‹¹ ì´ë¯¸ì§€ì—ì„œ ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ë“¤(ì¤‘ë³µ ì œê±°)ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì–»ìŒ\n","                mask = (classes == cls_id) # (M,) í´ë˜ìŠ¤ë³„ ë°•ìŠ¤ë§Œ ì„ íƒ\n","                cls_boxes  = boxes[mask]   # (m,4): (M, 4)ëŠ” ì•„ì§ ì–´ë–¤ í´ë˜ìŠ¤ ê¸°ì¤€ìœ¼ë¡œë„ ê±¸ëŸ¬ë‚´ì§€ ì•Šì€ ìƒíƒœ. (m, 4)ëŠ” 'ì§€ê¸ˆ ë³´ê³  ìˆëŠ” í´ë˜ìŠ¤'ì— ì†í•˜ëŠ” ë°•ìŠ¤ë§Œ ë‚¨ê¸´ ê²°ê³¼\n","                cls_scores = scores[mask]  # (m,): í´ë˜ìŠ¤ë³„ ë°•ìŠ¤ ìˆ˜\n","\n","                # ì˜ˆì‹œ\n","                # boxes = torch.tensor([\n","                #    [0,0,10,10],    # idx 0\n","                #    [1,1,11,11],    # idx 1\n","                #    [50,50,60,60],  # idx 2\n","                #    [70,70,80,80]   # idx 3\n","                # ])\n","                # scores = torch.tensor([0.9, 0.8, 0.7, 0.3])\n","                # keep = nms(boxes, scores, iou_thresh=0.4)\n","                # print(keep)  # tensor([0, 2, 3])\n","                # idxì˜ scoreê°€ 0.8ì¸ë°ë„ ì œê±°í•œ ì´ìœ ëŠ” ì ìˆ˜ê°€ ë†’ì€ 1ë³´ë‹¤ë„ ì ìˆ˜ê°€ ë§ê¸° ë•Œë¬¸ì„.\n","                # idx 0ì™€ idx 1ì˜ IoUëŠ” 0.68ë¡œ iou_thresh 0.4 ë³´ë‹¤ ë†’ìœ¼ë¯€ë¡œ 1ì´ ì œê±°ë¨\n","                # idx 3ì€ scoreê°€ 0.3ì´ì§€ë§Œ iouì˜ ëŒ€ìƒìì²´ê°€ ì•„ë‹ˆê¸°ì— ì¶œë ¥ì— í¬í•¨. IoUëŠ” ê²¹ì¹ ë•Œ ì¤‘ë³µëœ ìƒìë¥¼ ì‚­ì œí• ë•Œ ì“°ëŠ” ë¡œì§ì„\n","                keep_idxs  = nms(cls_boxes, cls_scores, iou_thresh) # ë°•ìŠ¤ë“¤ì˜ index ë¦¬ìŠ¤íŠ¸\n","\n","                if keep_idxs.numel() > 0:\n","                    cls_id_col = torch.full((keep_idxs.numel(),1), cls_id, dtype=boxes.dtype, device=boxes.device)\n","                    selected = torch.cat([\n","                        cls_boxes[keep_idxs], # (k, 4) -> k <= m\n","                        cls_scores[keep_idxs].unsqueeze(1), # (k, 1) -> k <= m\n","                        cls_id_col # (k, 1) -> k <= m\n","                    ], dim=1)  # dimì´ 1ì´ë¯€ë¡œ ì—´ë°©í–¥ìœ¼ë¡œ concatí•œë‹¤. (k,6)\n","                    kept.append(selected)\n","\n","            if kept:\n","                # if keptê°€ [torch.Size([2,6]), torch.Size([1,6]), torch.Size([4,6])] ë©´, batch_detectionsëŠ” torch.Size([7,6])\n","                batch_detections.append(torch.vstack(kept)) # kept ì•ˆì˜ [k,6] í…ì„œë¥¼ ì´ì–´ ë¶™ì—¬ [K,6] ìƒì„±\n","            else:\n","                batch_detections.append(torch.zeros((0,6), dtype=boxes.dtype, device=boxes.device))\n","        return batch_detections\n","\n","def load_model(model, name):\n","    ckpt_file = Path(f'./models/{name}.pth')\n","    if ckpt_file.is_file():\n","        state = torch.load(ckpt_file, map_location=torch.device(cfg.DEVICE))\n","        model.load_state_dict(state)\n","        print(f\"Loaded checkpoint from {ckpt_file}\")\n","    else:\n","        print(f\"No checkpoint at {ckpt_file}, skipping load\")\n","        input_file = Path(f'/kaggle/input/modelpth/{name}.pth')\n","        if input_file.is_file():\n","            state = torch.load(input_file, map_location=torch.device(cfg.DEVICE))\n","            model.load_state_dict(state)\n","            print(f\"Loaded input from {input_file}\")\n","        else:\n","            print(f\"No loaded input at {ckpt_file}, skipping load\")\n","    def save_model():\n","        os.makedirs('./models', exist_ok=True)\n","        torch.save(model.state_dict(), ckpt_file)\n","    return save_model\n","\n","\n","def draw_detections_pil(image_tensor, detections, class_names, output_path=None):\n","    \"\"\"\n","    image_tensor : torch.Tensor (3, H, W), float [0,1]\n","    detections   : torch.Tensor or array (K,6) [x1,y1,x2,y2,score,cls_idx], normalized\n","    class_names  : í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸\n","    \"\"\"\n","    # 1) Tensor â†’ HÃ—WÃ—3 uint8 â†’ PIL\n","    img_np = (image_tensor\n","              .mul(255)\n","              .clamp(0,255)\n","              .byte()\n","              .permute(1,2,0)\n","              .cpu()\n","              .numpy())\n","    pil_img = Image.fromarray(img_np)\n","    draw = ImageDraw.Draw(pil_img)\n","\n","    # 2) í°íŠ¸ ì¤€ë¹„\n","    try:\n","        font = ImageFont.truetype(\"arial.ttf\", size=16)\n","    except IOError:\n","        font = ImageFont.load_default()\n","\n","    W, H = pil_img.size\n","    dets = detections.detach().cpu().tolist()\n","\n","    for x1,y1,x2,y2,score,cls_idx in dets:\n","        # í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜\n","        x1p, y1p = int(x1*W), int(y1*H)\n","        x2p, y2p = int(x2*W), int(y2*H)\n","\n","        # 3) ë°”ìš´ë”© ë°•ìŠ¤\n","        draw.rectangle([x1p, y1p, x2p, y2p], outline=\"lime\", width=2)\n","\n","        # 4) ë¼ë²¨ ë¬¸ìì—´\n","        label = f\"{class_names[int(cls_idx)]}:{score:.2f}\"\n","\n","        # ğŸš© mask.size ë¡œ í…ìŠ¤íŠ¸ í¬ê¸° êµ¬í•˜ê¸°\n","        mask = font.getmask(label)\n","        tw, th = mask.size\n","\n","        # 5) ë¼ë²¨ ë°°ê²½ ê·¸ë¦¬ê¸°\n","        bg_xy = [x1p, y1p - th - 4, x1p + tw + 4, y1p]\n","        draw.rectangle(bg_xy, fill=\"black\")\n","\n","        # 6) í°ìƒ‰ í…ìŠ¤íŠ¸\n","        draw.text((x1p+2, y1p-th-2), label, font=font, fill=\"white\")\n","\n","    # 7) ì €ì¥ ë˜ëŠ” Matplotlib í‘œì‹œ\n","    if output_path:\n","        pil_img.save(output_path)\n","    else:\n","        plt.figure(figsize=(8,6))\n","        plt.imshow(pil_img)\n","        plt.axis(\"off\")\n","        plt.show()\n","\n","    return pil_img\n","\n","\n","# ì£¼ì–´ì§„ ë‘ ë°•ìŠ¤ì˜ ì¤‘ì‹¬ ì¢Œí‘œì™€ í¬ê¸°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢Œì¸¡ìƒë‹¨, ìš°ì¸¡í•˜ë‹¨ ì¢Œí‘œë¥¼ ê³„ì‚°í•˜ê³ , êµì§‘í•© ì˜ì—­ì„ í†µí•´ IoUë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n","# IoU ê³„ì‚° í•¨ìˆ˜ (YOLOv1ì—ì„œ ì‚¬ìš©í•˜ëŠ” bounding box í˜•ì‹: [x_center, y_center, width, height])\n","def iou_cxcywh(boxes1, boxes2, eps=1e-6):\n","    \"\"\"\n","    boxes1, boxes2: í…ì„œ, ë§ˆì§€ë§‰ ì°¨ì›ì´ [x_center, y_center, width, height]\n","    \"\"\"\n","    # ì¢Œì¸¡ ìƒë‹¨, ìš°ì¸¡ í•˜ë‹¨ ì¢Œí‘œ ê³„ì‚°. (x1,y1,x2,y2)ë¡œ ë³€í™˜\n","    box1_x1 = boxes1[:,0] - boxes1[:,2] / 2\n","    box1_y1 = boxes1[:,1] - boxes1[:,3] / 2\n","    box1_x2 = boxes1[:,0] + boxes1[:,2] / 2\n","    box1_y2 = boxes1[:,1] + boxes1[:,3] / 2\n","\n","    box2_x1 = boxes2[:,0] - boxes2[:,2] / 2\n","    box2_y1 = boxes2[:,1] - boxes2[:,3] / 2\n","    box2_x2 = boxes2[:,0] + boxes2[:,2] / 2\n","    box2_y2 = boxes2[:,1] + boxes2[:,3] / 2\n","\n","    # êµì§‘í•© ì˜ì—­\n","    x1 = torch.max(box1_x1, box2_x1)\n","    y1 = torch.max(box1_y1, box2_y1)\n","    x2 = torch.min(box1_x2, box2_x2)\n","    y2 = torch.min(box1_y2, box2_y2)\n","    inter_w  = (x2 - x1).clamp(min=0)\n","    inter_h  = (y2 - y1).clamp(min=0)\n","    inter = inter_w * inter_h\n","\n","    # í•©ì§‘í•© ì˜ì—­\n","    box1_area = torch.abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n","    box2_area = torch.abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n","    union = box1_area + box2_area - inter + eps\n","\n","    # IoU: Intersaction over Union\n","    iou_val = inter / union\n","    return iou_val\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T15:34:38.955239Z","iopub.execute_input":"2025-07-20T15:34:38.955698Z","iopub.status.idle":"2025-07-20T15:34:38.978216Z","shell.execute_reply.started":"2025-07-20T15:34:38.955669Z","shell.execute_reply":"2025-07-20T15:34:38.977458Z"},"jupyter":{"source_hidden":true},"id":"mwtV2WMNX1eU"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## 3. ëª¨ë¸ ì •ì˜"],"metadata":{"id":"rc6A0ehgX1eW"}},{"cell_type":"code","source":["# ------------------------------------------------------------------------------\n","# 1. Model Definition (YOLOv1)\n","# ------------------------------------------------------------------------------\n","class YOLOv1(nn.Module):\n","    # ë…¼ë¬¸ì— ì œì‹œëœ ì•„í‚¤í…ì²˜ êµ¬ì„± (YOLOv1)\n","    architecture_config = [\n","        (7, 64, 2, 3),       # (kernel_size, filters, stride, padding)\n","        \"M\",                 # maxpool\n","        (3, 192, 1, 1),\n","        \"M\",\n","        (1, 128, 1, 0),\n","        (3, 256, 1, 1),\n","        (1, 256, 1, 0),\n","        (3, 512, 1, 1),\n","        \"M\",\n","        [(1, 256, 1, 0), (3, 512, 1, 1), 4],  # í•´ë‹¹ ë¸”ë¡ì„ 4ë²ˆ ë°˜ë³µ\n","        (1, 512, 1, 0),\n","        (3, 1024, 1, 1),\n","        \"M\",\n","        [(1, 512, 1, 0), (3, 1024, 1, 1), 2],  # í•´ë‹¹ ë¸”ë¡ì„ 2ë²ˆ ë°˜ë³µ\n","        (3, 1024, 1, 1),\n","        (3, 1024, 2, 1),\n","        (3, 1024, 1, 1),\n","        (3, 1024, 1, 1)\n","    ]\n","\n","    def __init__(self, in_channels=3, S=7, B=2, C=20, conf_thresh=0.2, iou_thresh=0.4): # split_size=7, num_boxes=2, num_classes=20\n","        super(YOLOv1, self).__init__()\n","        self.S, self.B, self.C = S, B, C\n","        self.conf_thresh, self.iou_thresh = conf_thresh, iou_thresh\n","        self.features = YOLOv1.create_conv_layers(self.architecture_config, in_channels)\n","        # ì…ë ¥ ì´ë¯¸ì§€ê°€ 448x448ì¸ ê²½ìš°, ë§ˆì§€ë§‰ ì»¨ë³¼ë£¨ì…˜ feature mapì€ 7x7 (ë…¼ë¬¸ ê¸°ì¤€)\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(1024 * 7 * 7, 4096),\n","            nn.LeakyReLU(0.1),\n","            nn.Dropout(0.5),  # ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ dropout\n","            nn.Linear(4096, S * S * (C + B * 5))\n","        )\n","        self.loss_fn = YoloLoss(self.S, self.B, self.C)\n","\n","    def forward(self, x, targets=None):\n","        # x: [B,3,H,W] â†’ features â†’ [B,1024,S,S]\n","        x = self.features(x)\n","        # classifier â†’ [B, S*S*(5B + C)]\n","        x = self.classifier(x)\n","        # reshape â†’ [B, S, S, 5B + C]\n","        # print('YOLOv1 forward', x.view(-1, self.S, self.S, 5*self.B + self.C).size())\n","        # YOLOv1 forward torch.Size([16, 7, 7, 30])\n","        x = x.view(-1, self.S, self.S, 5*self.B + self.C) # ê³„ì‚° ë³µì¡ë„ë¥¼ ë‚®ì¶”ê¸° ìœ„í•´ (N, S, S, 5*B+C) í˜•íƒœë¡œ ë°˜í™˜í•œë‹¤.\n","\n","        preds = x\n","\n","        # â€” í™œì„±í™” ì ìš© â€”\n","        for b in range(self.B):\n","            off = 5*b\n","            # t_x, t_y â†’ sigmoid\n","            preds[..., off:off+2]   = torch.sigmoid(preds[..., off:off+2])\n","            # confidence â†’ sigmoid\n","            preds[..., off+4:off+5] = torch.sigmoid(preds[..., off+4:off+5])\n","\n","        # class logits â†’ softmax\n","        preds[..., 5*self.B : ] = F.softmax(preds[..., 5*self.B : ], dim=-1)\n","        return preds\n","\n","        # if targets is not None:\n","        #     #perfect_preds = targets.clone()       # predì™€ target í¬ë§·ì´ ë™ì¼í•´ì•¼ í•¨\n","        #     #return self.loss_fn(perfect_preds, targets)\n","        #     return self.loss_fn(x, targets) # tensor scalar ê°’ì„\n","\n","        # # inference ëª¨ë“œ\n","        # return YOLOv1.postprocess(x, self.conf_thresh, self.iou_thresh, self.S, self.B, self.C, flatten), x\n","\n","    @staticmethod\n","    def create_conv_layers(config, in_channels):\n","        layers = []\n","        for module in config:\n","            if type(module) == tuple:\n","                # íŠœí”Œ í˜•íƒœ: (kernel_size, filters, stride, padding)\n","                kernel_size, filters, stride, padding = module\n","                layers.append(nn.Conv2d(in_channels, filters, kernel_size, stride, padding))\n","                layers.append(nn.LeakyReLU(0.1))\n","                in_channels = filters\n","            elif module == \"M\":\n","                layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n","            elif type(module) == list:\n","                # ë¦¬ìŠ¤íŠ¸ í˜•íƒœ: [ conv1 íŠœí”Œ, conv2 íŠœí”Œ, ë°˜ë³µ íšŸìˆ˜ ]\n","                conv1, conv2, num_repeats = module\n","                for _ in range(num_repeats):\n","                    # ì²« ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜\n","                    k, f, s, p = conv1\n","                    layers.append(nn.Conv2d(in_channels, f, k, s, p))\n","                    layers.append(nn.LeakyReLU(0.1))\n","                    in_channels = f\n","                    # ë‘ ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜\n","                    k, f, s, p = conv2\n","                    layers.append(nn.Conv2d(in_channels, f, k, s, p))\n","                    layers.append(nn.LeakyReLU(0.1))\n","                    in_channels = f\n","        return nn.Sequential(*layers)\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# 2. YOLOv1 ì†ì‹¤ í•¨ìˆ˜ (ì±„ë„ ë§¤í•‘: 0â€“4 box1,4 conf1,5â€“9 box2,9 conf2,10â€“29 class)\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","def YoloLoss(S, B, C, lambda_coord=5.0, lambda_noobj=0.5):\n","    def iou_xyxy(boxes1, boxes2):\n","        x11, y11, x12, y12 = boxes1.unbind(-1)\n","        x21, y21, x22, y22 = boxes2.unbind(-1)\n","\n","        inter_x1 = torch.max(x11, x21)\n","        inter_y1 = torch.max(y11, y21)\n","        inter_x2 = torch.min(x12, x22)\n","        inter_y2 = torch.min(y12, y22)\n","\n","        inter_w = (inter_x2 - inter_x1).clamp(min=0)\n","        inter_h = (inter_y2 - inter_y1).clamp(min=0)\n","        inter_area = inter_w * inter_h\n","\n","        area1 = (x12 - x11) * (y12 - y11)\n","        area2 = (x22 - x21) * (y22 - y21)\n","        union = area1 + area2 - inter_area\n","        return inter_area / union.clamp(min=1e-6)\n","\n","    def xywh_to_xyxy(box):\n","        cx, cy, w, h = box.unbind(-1)\n","        x1 = cx - w/2\n","        y1 = cy - h/2\n","        x2 = cx + w/2\n","        y2 = cy + h/2\n","        return torch.stack([x1, y1, x2, y2], dim=-1)\n","\n","    def yolo_loss(predictions, target):\n","                  # S=7, B=2, C=20,\n","                  #lambda_coord=5.0, lambda_noobj=0.5):\n","        \"\"\"\n","        predictions: (batch, S*S*(5*B + C))\n","        target:      (batch, S, S, 5*B + C)\n","        \"\"\"\n","        device = predictions.device\n","\n","        # 1) reshape to (batch, S, S, 5*B + C)\n","        preds = predictions.view(-1, S, S, 5*B + C)\n","\n","        # 2) ê·¸ë¦¬ë“œ ì˜¤í”„ì…‹ ìƒì„±\n","        grid_y, grid_x = torch.meshgrid(\n","            torch.arange(S, device=device),\n","            torch.arange(S, device=device),\n","            indexing='ij'\n","        )\n","        grid = torch.stack([grid_x, grid_y], dim=-1).view(1, S, S, 1, 2)\n","\n","        # 3) ë‘ ë°•ìŠ¤ ìŠ¬ë¡¯ (cx,cy,w,h) ì¶”ì¶œ\n","        rel1 = preds[...,  0:4].view(-1, S, S, 1, 4)\n","        rel2 = preds[...,  5:9].view(-1, S, S, 1, 4)\n","\n","        # 4) ì…€ ë‚´ ìƒëŒ€ â†’ ì ˆëŒ€ì¢Œí‘œ [0..1] ë””ì½”ë”©\n","        abs1 = torch.cat([\n","            (grid + rel1[..., :2]) / S,    # center\n","            rel1[..., 2:4]                 # w,h\n","        ], dim=-1)  # (batch, S, S, 1, 4)\n","\n","        abs2 = torch.cat([\n","            (grid + rel2[..., :2]) / S,\n","            rel2[..., 2:4]\n","        ], dim=-1)\n","\n","        # 5) GT ë°•ìŠ¤ë„ ë™ì¼í•˜ê²Œ ë””ì½”ë”© (target[...,0:4])\n","        gt_rel = target[..., 0:4].view(-1, S, S, 1, 4)\n","        gt_abs = torch.cat([\n","            (grid + gt_rel[..., :2]) / S,\n","            gt_rel[..., 2:4]\n","        ], dim=-1)\n","\n","        bb1 = xywh_to_xyxy(abs1).squeeze(3)  # (batch, S, S, 4)\n","        bb2 = xywh_to_xyxy(abs2).squeeze(3)\n","        gt  = xywh_to_xyxy(gt_abs).squeeze(3)\n","\n","        # 6) IoU ê³„ì‚° & ì±…ì„ ë°•ìŠ¤ ì„ ì •\n","        iou_b1 = iou_xyxy(bb1, gt)\n","        iou_b2 = iou_xyxy(bb2, gt)\n","        ious   = torch.stack([iou_b1, iou_b2], dim=0)       # (2, batch, S, S)\n","        iou_maxes, bestbox = torch.max(ious, dim=0)         # (batch,S,S)\n","        bestbox = bestbox.unsqueeze(-1).float()             # (batch,S,S,1)\n","\n","        # 7) object mask (conf1 ìë¦¬: channel 4)\n","        exists_box = target[..., 4].unsqueeze(-1)           # (batch,S,S,1)\n","\n","        # 8) Box ì¢Œí‘œ ì†ì‹¤\n","        box_pred = exists_box * (\n","            bestbox * preds[..., 5:9] +      # slot2 coords\n","            (1-bestbox) * preds[..., 0:4]    # slot1 coords\n","        )\n","        box_tgt  = exists_box * target[...,  0:4]\n","\n","        # sqrt(w), sqrt(h)\n","        p_wh = torch.sqrt(box_pred[..., 2:4].clamp(min=1e-6))\n","        t_wh = torch.sqrt(box_tgt[...,  2:4].clamp(min=1e-6))\n","        box_predxy  = torch.cat([box_pred[..., :2], p_wh], dim=-1)\n","        box_target  = torch.cat([box_tgt[...,  :2], t_wh], dim=-1)\n","\n","        box_loss = torch.sum((box_predxy - box_target) ** 2)\n","\n","        # 9) Object confidence loss\n","        pred_conf   = bestbox * preds[..., 9:10] + (1-bestbox) * preds[..., 4:5]\n","        conf_target = exists_box * iou_maxes.unsqueeze(-1)\n","        obj_conf_loss = torch.sum((exists_box * (pred_conf - conf_target)) ** 2)\n","\n","        # 10) No-object confidence loss\n","        noobj_mask  = 1 - exists_box\n","        noobj_loss  = torch.sum((noobj_mask * preds[..., 4:5]) ** 2)\n","        noobj_loss += torch.sum((noobj_mask * preds[..., 9:10]) ** 2)\n","\n","        # 11) Class probability loss (channels 10~10+C)\n","        cls_pred   = preds[..., 10:10+C]\n","        cls_target = target[...,10:10+C]\n","        class_loss = torch.sum((exists_box * (cls_pred - cls_target)) ** 2)\n","\n","        # 12) ì´í•©\n","        total_loss = (\n","            lambda_coord * box_loss +\n","            obj_conf_loss +\n","            lambda_noobj * noobj_loss +\n","            class_loss\n","        )\n","        batch_size = predictions.size(0)\n","        return total_loss / batch_size # ìˆ˜ì •1: ë°°ì¹˜ ì‚¬ì´ì¦ˆë¡œ ë‚˜ëˆ ì¤€ë‹¤. original: return total_loss\n","    return yolo_loss\n","\n","\n","# ------------------------------------------------------------------------------\n","# 3. Training & Validation Loop\n","# ------------------------------------------------------------------------------\n","def train_one_epoch(model, loader, opt, scheduler, device):\n","    model.train()\n","    total_loss = 0\n","    for imgs, targets, metas in tqdm(loader, desc='Train batchs'):\n","        imgs, targets = imgs.to(device), targets.to(device)\n","        #loss = model(imgs, targets)\n","        preds = model(imgs)\n","        loss = model.loss_fn(preds, targets)\n","        # print('train matas', metas)\n","        # print('train_one_epoch', preds.shape)\n","        # loss = loss_fn(preds, targets)\n","        opt.zero_grad()\n","        loss.backward()\n","        opt.step()\n","        # scheduler.step()\n","        total_loss += loss.item()\n","    return total_loss / len(loader)\n","\n","def validate(model, loader, device):\n","    model.eval()\n","    total_loss = 0\n","    with torch.no_grad():\n","        for imgs, targets, metas in loader:\n","            imgs, targets = imgs.to(device), targets.to(device)\n","            #loss = model(imgs, targets)\n","            preds = model(imgs)\n","            loss = model.loss_fn(preds, targets)\n","\n","            # print('val matas', metas)\n","            # print('loss: ', loss)\n","            # loss = loss_fn(preds, targets)\n","            total_loss += loss.item()\n","    return total_loss / len(loader)"],"metadata":{"trusted":true,"id":"fUEpRr89X1eW"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## 4. í›ˆë ¨ ë° ì¶”ë¡ "],"metadata":{"id":"Yu0lH1i9X1eY"}},{"cell_type":"code","source":["# ------------------------------------------------------------------------------\n","# 4. Main: í•™ìŠµ ë° ì¶”ë¡  ì˜ˆì‹œ\n","# ------------------------------------------------------------------------------\n","\n","# ëª¨ë¸Â·ì†ì‹¤Â·ìµœì í™”ê¸°\n","model = YOLOv1(cfg.IMAGE_CH_SIZE, cfg.S, cfg.B, cfg.C, cfg.CONF_THRESHOLD, cfg.NMS_IOU_THRESH).to(cfg.DEVICE)\n","save_model = load_model(model, 'model3')\n","#opt = optim.Adam(model.parameters(), lr=cfg.LR) #, weight_decay=cfg.WD\n","opt = torch.optim.SGD(model.parameters(), lr=cfg.LR, momentum=0.9, weight_decay=cfg.WD)\n","total_steps = cfg.EPOCHS * len(train_loader)\n","scheduler = OneCycleLR(\n","    opt,\n","    max_lr=1e-2,\n","    total_steps=total_steps,\n","    pct_start=0.1,  # warmup ë¹„ìœ¨\n","    anneal_strategy='cos'\n",")\n","\n","# í•™ìŠµ ë£¨í”„\n","for epoch in range(1, cfg.EPOCHS+1):\n","    train_loss = train_one_epoch(model, train_loader, opt, scheduler, cfg.DEVICE)\n","    val_loss   = validate(model, val_loader, cfg.DEVICE)\n","    print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n","    if epoch % 5 == 0:\n","        save_model()\n","\n","# ì¶”ë¡  ì˜ˆì‹œ\n","model.eval()\n","with torch.no_grad():\n","    test_img, _, meta = val_ds[0]\n","    detections = model(test_img.unsqueeze(0).to(cfg.DEVICE))\n","    print(\"Sample detections:\", detections[0])\n","    print(\"meta:\", meta)\n","\n","'''\n","Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.29it/s]\n","Epoch 01 | Train Loss: 7.2043 | Val Loss: 6.7486\n","Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.31it/s]\n","Epoch 02 | Train Loss: 7.0637 | Val Loss: 6.5826\n","Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.32it/s]\n","Epoch 03 | Train Loss: 6.7607 | Val Loss: 6.4628\n","Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.31it/s]\n","Epoch 04 | Train Loss: 6.7036 | Val Loss: 6.5068\n","Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.30it/s]\n","Epoch 05 | Train Loss: 6.7157 | Val Loss: 6.3505\n","Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.28it/s]\n","Epoch 06 | Train Loss: 6.7842 | Val Loss: 6.4915\n","Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.31it/s]\n","Epoch 07 | Train Loss: 6.6331 | Val Loss: 6.4501\n","Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.32it/s]\n","Epoch 08 | Train Loss: 6.6444 | Val Loss: 6.2640\n","Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.31it/s]\n","Epoch 09 | Train Loss: 6.5844 | Val Loss: 6.2967\n","Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.31it/s]\n","Epoch 10 | Train Loss: 6.5646 | Val Loss: 6.3164\n","Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.31it/s]\n","Epoch 11 | Train Loss: 6.5353 | Val Loss: 6.2102\n","Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.31it/s]\n","Epoch 12 | Train Loss: 6.9539 | Val Loss: 7.7272\n","Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.28it/s]\n","'''\n","print()\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T15:44:27.647038Z","iopub.execute_input":"2025-07-20T15:44:27.647332Z","execution_failed":"2025-07-20T23:25:19.072Z"},"id":"XCDl8VPwX1eZ","outputId":"3f5d5d42-2173-4b06-8ad9-f2d89196f494"},"outputs":[{"name":"stdout","text":"Loaded checkpoint from models/model3.pth\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | Train Loss: 7.2043 | Val Loss: 6.7486\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 02 | Train Loss: 7.0637 | Val Loss: 6.5826\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 03 | Train Loss: 6.7607 | Val Loss: 6.4628\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 04 | Train Loss: 6.7036 | Val Loss: 6.5068\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 05 | Train Loss: 6.7157 | Val Loss: 6.3505\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 06 | Train Loss: 6.7842 | Val Loss: 6.4915\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 07 | Train Loss: 6.6331 | Val Loss: 6.4501\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 08 | Train Loss: 6.6444 | Val Loss: 6.2640\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 09 | Train Loss: 6.5844 | Val Loss: 6.2967\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss: 6.5646 | Val Loss: 6.3164\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | Train Loss: 6.5353 | Val Loss: 6.2102\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | Train Loss: 6.9539 | Val Loss: 7.7272\n","output_type":"stream"},{"name":"stderr","text":"Train batchs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.28it/s]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["### Inference ë¡œì§ ###\n","\n","model = YOLOv1(cfg.IMAGE_CH_SIZE, cfg.S, cfg.B, cfg.C, cfg.CONF_THRESHOLD, cfg.NMS_IOU_THRESH).to(cfg.DEVICE)\n","save_model = load_model(model, 'model1')\n","\n","# ì¶”ë¡  ì˜ˆì‹œ\n","model.eval()\n","with torch.no_grad():\n","    for i in range(10): # len(val_ds)\n","        test_img, _, meta = val_ds[i] # train_ds[i] #val_ds[i]\n","        print(test_img.size())\n","        preds = model(test_img.unsqueeze(0).to(cfg.DEVICE))\n","        detections = Utils.postprocess(preds, cfg.CONF_THRESHOLD, cfg.NMS_IOU_THRESH, cfg.S, cfg.B, cfg.C)\n","        draw_detections_pil(test_img, detections[0], VOC_CLASSES)\n","        print(\"Sample detections:\", detections[0])\n","        print('meta: ', meta)"],"metadata":{"trusted":true,"id":"jKwUYf-gX1ea"},"outputs":[],"execution_count":null}]}